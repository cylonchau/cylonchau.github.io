<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ceph RBD - 初识块存储RBD | Cylon's Collection</title><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NP3JNCPR" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><meta name=keywords content="ceph rbd,block device"><meta name=description content="什么是块存储 RBD Ceph RBD (RADOS Block Device) 是 Ceph 提供的三种存储类型之一 (块存储 RBD, 文件存储 CephFS, 对象存储 RGW)，也是另外两个存储类型 (文件存储 CephFS, 对象存储 RGW) 的底座，位于 RADOS 架构中的最底层，由下图可以看出
图：Ceph RADOS架构图Source：https://www.supportsages.com/ceph-part-3-technical-architecture-and-components/
RADOS 是可信赖的自动分布式对象存储 (Reliable Autonomous Distributed Object Store) 的简写，通俗来说，RADOS 代表的就是整个 Ceph 集群，数据对象在集群中的存储方式会“将对象复制为多副本” 以实现容错，所以 Ceph 集群的底座就是 RADOS，一个 RADOS 集群的组件通常包含三个，OSD Daemon , MDS, MON
Object Storage Device (OSD) Daemon：RADOS集群中负责存储守护进程，与 OSD (数据的物理或逻辑存储单元【通常指一个硬盘】)交互。集群中的每个 Ceph Node 都必须运行 OSD Daemon。对于每个 OSD，可以有一个关联的硬盘 (通常一个OSD Daemon 对应一个存储单元)。 MONITORS (Mon Daemon)：Monitor (ceph-mon) 不是集群存储组件的一部分，但它通过监视 OSD 状态并生成 “Cluster Map” 而成为 RADOS 不可或缺的一部分。它监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。一般来说，它充当存储集群中所有 OSD 的 Monitor Manager (MGR Daemon)：Manager (ceph-mgr) 是与 ceph-mon 一同运行的守护进程，为外部监控和管理系统提供额外的监视和接口。默认情况下，ceph-mgr 除了确保其正在运行之外不需要其他配置。如果没有运行 ceph-mgr，ceph -s 将会看到一条 WARN；不管是使用什么方式部署的集群 ( ceph-deploy, cephadm)，ceph-mgr 总会 与 ceph-mon 同时运行在一个节点上，也可单独运行在 Ceph Node 之上。 通常 Monitor (ceph-mon) 不构成“存储”集群的一部分，只是通过监视 OSD 状态并生成 Cluster map 而成为 ceph存储集群中不可缺少的组件。它通过监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。"><meta name=author content="cylon"><link rel=canonical href=https://www.oomkill.com/2019/09/03-1-acquaintance-rdb/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://www.oomkill.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.oomkill.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.oomkill.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.oomkill.com/favicon.ico><link rel=mask-icon href=https://www.oomkill.com/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://www.oomkill.com/2019/09/03-1-acquaintance-rdb/><noscript><style>#theme-toggle,#top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=/assets/css/pe.min.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/pe.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/v4-shims.min.css><script defer src=https://cdn.staticfile.net/jquery/3.5.1/jquery.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/fancybox/3.5.7/jquery.fancybox.min.css><script defer src=https://cdn.staticfile.net/fancybox/3.5.7/jquery.fancybox.min.js></script><script id=MathJax-script async src=https://cdn.staticfile.net/mathjax/3.2.2/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["\\$","\\$"]]}}</script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script>(function(e,t,n,s,o){e[s]=e[s]||[],e[s].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var a=t.getElementsByTagName(n)[0],i=t.createElement(n),r=s!="dataLayer"?"&l="+s:"";i.async=!0,i.src="https://www.googletagmanager.com/gtm.js?id="+o+r,a.parentNode.insertBefore(i,a)})(window,document,"script","dataLayer","GTM-NP3JNCPR")</script><meta property="og:title" content="Ceph RBD - 初识块存储RBD"><meta property="og:description" content="什么是块存储 RBD Ceph RBD (RADOS Block Device) 是 Ceph 提供的三种存储类型之一 (块存储 RBD, 文件存储 CephFS, 对象存储 RGW)，也是另外两个存储类型 (文件存储 CephFS, 对象存储 RGW) 的底座，位于 RADOS 架构中的最底层，由下图可以看出
图：Ceph RADOS架构图Source：https://www.supportsages.com/ceph-part-3-technical-architecture-and-components/
RADOS 是可信赖的自动分布式对象存储 (Reliable Autonomous Distributed Object Store) 的简写，通俗来说，RADOS 代表的就是整个 Ceph 集群，数据对象在集群中的存储方式会“将对象复制为多副本” 以实现容错，所以 Ceph 集群的底座就是 RADOS，一个 RADOS 集群的组件通常包含三个，OSD Daemon , MDS, MON
Object Storage Device (OSD) Daemon：RADOS集群中负责存储守护进程，与 OSD (数据的物理或逻辑存储单元【通常指一个硬盘】)交互。集群中的每个 Ceph Node 都必须运行 OSD Daemon。对于每个 OSD，可以有一个关联的硬盘 (通常一个OSD Daemon 对应一个存储单元)。 MONITORS (Mon Daemon)：Monitor (ceph-mon) 不是集群存储组件的一部分，但它通过监视 OSD 状态并生成 “Cluster Map” 而成为 RADOS 不可或缺的一部分。它监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。一般来说，它充当存储集群中所有 OSD 的 Monitor Manager (MGR Daemon)：Manager (ceph-mgr) 是与 ceph-mon 一同运行的守护进程，为外部监控和管理系统提供额外的监视和接口。默认情况下，ceph-mgr 除了确保其正在运行之外不需要其他配置。如果没有运行 ceph-mgr，ceph -s 将会看到一条 WARN；不管是使用什么方式部署的集群 ( ceph-deploy, cephadm)，ceph-mgr 总会 与 ceph-mon 同时运行在一个节点上，也可单独运行在 Ceph Node 之上。 通常 Monitor (ceph-mon) 不构成“存储”集群的一部分，只是通过监视 OSD 状态并生成 Cluster map 而成为 ceph存储集群中不可缺少的组件。它通过监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。"><meta property="og:type" content="article"><meta property="og:url" content="https://www.oomkill.com/2019/09/03-1-acquaintance-rdb/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-09-30T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-06T23:10:36+08:00"><meta property="og:site_name" content="Cylon's Collection"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ceph RBD - 初识块存储RBD"><meta name=twitter:description content="什么是块存储 RBD Ceph RBD (RADOS Block Device) 是 Ceph 提供的三种存储类型之一 (块存储 RBD, 文件存储 CephFS, 对象存储 RGW)，也是另外两个存储类型 (文件存储 CephFS, 对象存储 RGW) 的底座，位于 RADOS 架构中的最底层，由下图可以看出
图：Ceph RADOS架构图Source：https://www.supportsages.com/ceph-part-3-technical-architecture-and-components/
RADOS 是可信赖的自动分布式对象存储 (Reliable Autonomous Distributed Object Store) 的简写，通俗来说，RADOS 代表的就是整个 Ceph 集群，数据对象在集群中的存储方式会“将对象复制为多副本” 以实现容错，所以 Ceph 集群的底座就是 RADOS，一个 RADOS 集群的组件通常包含三个，OSD Daemon , MDS, MON
Object Storage Device (OSD) Daemon：RADOS集群中负责存储守护进程，与 OSD (数据的物理或逻辑存储单元【通常指一个硬盘】)交互。集群中的每个 Ceph Node 都必须运行 OSD Daemon。对于每个 OSD，可以有一个关联的硬盘 (通常一个OSD Daemon 对应一个存储单元)。 MONITORS (Mon Daemon)：Monitor (ceph-mon) 不是集群存储组件的一部分，但它通过监视 OSD 状态并生成 “Cluster Map” 而成为 RADOS 不可或缺的一部分。它监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。一般来说，它充当存储集群中所有 OSD 的 Monitor Manager (MGR Daemon)：Manager (ceph-mgr) 是与 ceph-mon 一同运行的守护进程，为外部监控和管理系统提供额外的监视和接口。默认情况下，ceph-mgr 除了确保其正在运行之外不需要其他配置。如果没有运行 ceph-mgr，ceph -s 将会看到一条 WARN；不管是使用什么方式部署的集群 ( ceph-deploy, cephadm)，ceph-mgr 总会 与 ceph-mon 同时运行在一个节点上，也可单独运行在 Ceph Node 之上。 通常 Monitor (ceph-mon) 不构成“存储”集群的一部分，只是通过监视 OSD 状态并生成 Cluster map 而成为 ceph存储集群中不可缺少的组件。它通过监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.oomkill.com/posts/"},{"@type":"ListItem","position":2,"name":"Ceph RBD - 初识块存储RBD","item":"https://www.oomkill.com/2019/09/03-1-acquaintance-rdb/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ceph RBD - 初识块存储RBD","name":"Ceph RBD - 初识块存储RBD","description":"什么是块存储 RBD Ceph RBD (RADOS Block Device) 是 Ceph 提供的三种存储类型之一 (块存储 RBD, 文件存储 CephFS, 对象存储 RGW)，也是另外两个存储类型 (文件存储 CephFS, 对象存储 RGW) 的底座，位于 RADOS 架构中的最底层，由下图可以看出\n图：Ceph RADOS架构图\rSource：https://www.supportsages.com/ceph-part-3-technical-architecture-and-components/\nRADOS 是可信赖的自动分布式对象存储 (Reliable Autonomous Distributed Object Store) 的简写，通俗来说，RADOS 代表的就是整个 Ceph 集群，数据对象在集群中的存储方式会“将对象复制为多副本” 以实现容错，所以 Ceph 集群的底座就是 RADOS，一个 RADOS 集群的组件通常包含三个，OSD Daemon , MDS, MON\nObject Storage Device (OSD) Daemon：RADOS集群中负责存储守护进程，与 OSD (数据的物理或逻辑存储单元【通常指一个硬盘】)交互。集群中的每个 Ceph Node 都必须运行 OSD Daemon。对于每个 OSD，可以有一个关联的硬盘 (通常一个OSD Daemon 对应一个存储单元)。 MONITORS (Mon Daemon)：Monitor (ceph-mon) 不是集群存储组件的一部分，但它通过监视 OSD 状态并生成 “Cluster Map” 而成为 RADOS 不可或缺的一部分。它监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。一般来说，它充当存储集群中所有 OSD 的 Monitor Manager (MGR Daemon)：Manager (ceph-mgr) 是与 ceph-mon 一同运行的守护进程，为外部监控和管理系统提供额外的监视和接口。默认情况下，ceph-mgr 除了确保其正在运行之外不需要其他配置。如果没有运行 ceph-mgr，ceph -s 将会看到一条 WARN；不管是使用什么方式部署的集群 ( ceph-deploy, cephadm)，ceph-mgr 总会 与 ceph-mon 同时运行在一个节点上，也可单独运行在 Ceph Node 之上。 通常 Monitor (ceph-mon) 不构成“存储”集群的一部分，只是通过监视 OSD 状态并生成 Cluster map 而成为 ceph存储集群中不可缺少的组件。它通过监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。","keywords":["ceph rbd","block device"],"articleBody":"什么是块存储 RBD Ceph RBD (RADOS Block Device) 是 Ceph 提供的三种存储类型之一 (块存储 RBD, 文件存储 CephFS, 对象存储 RGW)，也是另外两个存储类型 (文件存储 CephFS, 对象存储 RGW) 的底座，位于 RADOS 架构中的最底层，由下图可以看出\n图：Ceph RADOS架构图\rSource：https://www.supportsages.com/ceph-part-3-technical-architecture-and-components/\nRADOS 是可信赖的自动分布式对象存储 (Reliable Autonomous Distributed Object Store) 的简写，通俗来说，RADOS 代表的就是整个 Ceph 集群，数据对象在集群中的存储方式会“将对象复制为多副本” 以实现容错，所以 Ceph 集群的底座就是 RADOS，一个 RADOS 集群的组件通常包含三个，OSD Daemon , MDS, MON\nObject Storage Device (OSD) Daemon：RADOS集群中负责存储守护进程，与 OSD (数据的物理或逻辑存储单元【通常指一个硬盘】)交互。集群中的每个 Ceph Node 都必须运行 OSD Daemon。对于每个 OSD，可以有一个关联的硬盘 (通常一个OSD Daemon 对应一个存储单元)。 MONITORS (Mon Daemon)：Monitor (ceph-mon) 不是集群存储组件的一部分，但它通过监视 OSD 状态并生成 “Cluster Map” 而成为 RADOS 不可或缺的一部分。它监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。一般来说，它充当存储集群中所有 OSD 的 Monitor Manager (MGR Daemon)：Manager (ceph-mgr) 是与 ceph-mon 一同运行的守护进程，为外部监控和管理系统提供额外的监视和接口。默认情况下，ceph-mgr 除了确保其正在运行之外不需要其他配置。如果没有运行 ceph-mgr，ceph -s 将会看到一条 WARN；不管是使用什么方式部署的集群 ( ceph-deploy, cephadm)，ceph-mgr 总会 与 ceph-mon 同时运行在一个节点上，也可单独运行在 Ceph Node 之上。 通常 Monitor (ceph-mon) 不构成“存储”集群的一部分，只是通过监视 OSD 状态并生成 Cluster map 而成为 ceph存储集群中不可缺少的组件。它通过监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。\n如何确定 ceph-mon 的数量 ceph-mon 的数量最好是奇数，并且数量个数时有限限制的，这里在总结一下 ceph monitor 的作用：\nMonitor 不向客户端提供存储的对象，由于它只是一个监控节点，因此它不保存/管理任何对象，也不属于对象存储的一部分，但它仍然是 RADOS 的一部分，但不用于数据/对象存储。 它维护簇映射状态，即：存储集群客户端/应用程序从 ceph-mon 检索集群映射的副本 通常很奇怪并且数量有限，因为他们的工作相当简单，维护 OSD 的状态。 为分布式决策提供共识，当要针对 OSD 的状态做出特定决策时，它提供了一般规则。 当多个 OSD 要进行对等互连或复制时，监视器会自行决定如何进行对等互连，而不是由 ODS 自行决定。 Ceph OSD 守护进程检查自身状态和其他 OSD 的状态并向监视器报告。 中国铁路2020年分享的 1550+ OSD PB 级别存储 ceph-con 数量从 “3” 升级到 “5” 后，趋势稳定 [1]\nRBD 存储单元 RBD 块设备 (RADOS Block Device) 在 Ceph 中被称为 image。image 由 “元数据” 和 ”数据“两部分组成，其中元数据存储在多个特殊的 RADOS 对象中，而数据被自动”条带化“成多个 RADOS 对象进行存储。除了 image 自身的元数据之外，在 image 所属的 存储池 (Pool) 中都还有一组特殊的 RADOS 对象记录 image 关联关系或附加信息等相关的 RBD 管理元数据。所有的数据对象和元数据对象都依据 CRUSH 规则 (CRUSH RULE) 存储在底层的 OSD 设备上，因此 RBD 块设备自动继承了 RADOS 对象的数据冗余保护机制和一致性策略。\n对于 RBD Image 官方是这么描述的 [2]\nRBD images are simple block devices that are striped over objects and stored in a RADOS object store. The size of the objects the image is striped over must be a power of two.\n由此，我们可以得知，”镜像“ (RBD IMAGE) 的表现就是一个块设备\nPOOL 存储池是用于存储对象的逻辑分区，一个存储池中可以放置多个”IMAGE“。在官方给出的，存储池提供了如下特性:\n池提供以下功能：\n冗余性 (Resilience)：可以设置允许失败的 OSD 数量，而不会丢失任何数据。如果您的集群使用 ”复制池“ (replicated )，那么可以容忍的 OSD 故障数量等于复制的数量。\n例如：典型配置存储一个对象及其每个 RADOS 对象的两个副本（即size=3，默认值），但您可以根据池的需求配置副本数量。对于纠删码池 (erasure-coded)，冗余性定义为编码块的数量（例如，默认纠删码配置文件中的m = 2）。\n放置组（Placement Groups，PGs）：您可以为池设置放置组（PGs）的数量。在典型配置中，每个 OSD 的目标 PG 数量约为 100 个 PGs。这提供了合理的负载均衡，而不会消耗过多的计算资源。在设置多个存储池时，请小心为每个存储池和整个集群设置合适数量的 PGs。每个 PG 都属于特定的存储池：当多个存储池使用相同的 OSDs 时，请确保每个 OSD 上的 PG 副本总数在所需的 “每个 OSD 上 PG数量的目标范围内”。要计算适合您的池的PG数量，请使用 pgcalc [3] 工具。\nCRUSH规则：当数据存储在池中时，对象及其副本（或在纠删码池中的块）在您的集群中的放置由 CRUSH 规则管理。如果默认规则不适合您的用例，可以为池创建自定义 CRUSH 规则。\n快照：命令 ceph osd pool mksnap 可创建基于存储池的快照。\nlibrdb 和 librados 要想使用块设备，就涉及到 RADOS 之上数据的交互，我们已经了解到了 RBD 是为 KVM 等虑拟化技术和云 OS（如 OpenStack 和 CloudStack）提供高性能和无限可扩展性的存储后端，这些系统依赖于 libvirt 和 QEMU 实用程序与 RBD 进行集成。\n在Ceph中，提供了一种 librados 的库，它可以使得 客户端 与 Ceph 集群间的交互，在 RADOS 之上 存在一个 “librados\"库，而 “librbd” 库则是构建与 “librados” 之上的的库，提供了块存储的功能，下面是两个库在“用途”和“功能”上有一些重要区别：\nlibrados（RADOS客户端库）： 用途：librados是用于与Ceph的底层RADOS存储层交互的库。它提供了与Ceph集群中的对象存储进行直接通信的API，允许应用程序执行各种存储操作，如读取、写入、删除和管理存储对象。 功能：librados允许应用程序直接访问Ceph集群，而不需要高级抽象，这意味着应用程序可以更精细地控制数据的读写和存储策略。librados可以用于构建自定义数据存储和管理解决方案。 librbd（RBD客户端库）： 用途：librbd是用于与Ceph中的RBD（Rados Block Device）存储层交互的库。它构建在librados之上，提供了更高级别的抽象，用于处理块设备（镜像）操作，如创建、映射、快照和克隆等。 功能：librbd简化了块设备操作，并提供了易于使用的API，使应用程序能够轻松地管理RBD镜像。它是构建虚拟化平台、云存储、容器存储等应用的关键组件。 要使用 RBD 存储池需要先启用，而却需要注意的是 rbd 存储池并不能直接用于块设备，而是需要事先在其中按需创建映像（image），而 “映像” 代表了真正的块设备\nRBB 支持的功能 快照 排他锁/独占锁 镜像 实时迁移 .. 快照 RBD 快照 (snapshot) 是 image 在某个特定时间点的只读逻辑副本，类似于虚拟机的快照，快照也是 RBD 的高级功能之一，使得可以用户创建映像快照以保留时间点状态历史记录。 Ceph还支持快照分层，使您可以快速轻松地克隆镜像（例如虚拟机镜像）。 Ceph RBD 快照的使用通过命令 rbd 和几个更高级的接口进行管理，包括 QEMU, libvirt, OpenStack 和 CloudStack。\n由于 Ceph RBD 不知道 “镜像” 内的任何文件系统，因此快照仅是崩溃一致的，除非它们在挂载(mouting) 操作系统内进行协调。因此，我们建议您在拍摄快照之前暂停或停止 I/O。\n如果镜像包含文件系统，则在拍摄快照之前文件系统应处于内部一致状态。即需要停止IO\n快照的操作\nceph 有专门的命令 rbd 可以进行 RBD 相关的操作，例如快照的命令为 rbd snapshot\n命令 语法 样例 创建快照 rbd snap create {pool-name}/{image-name}@{snap-name} rbd snap create rbd/foo@snapname 列出快照 rbd snap ls {pool-name}/{image-name} rbd snap ls rbd/foo 回滚快照 rbd snap rollback {pool-name}/{image-name}@{snap-name} rbd snap rollback rbd/foo@snapname 删除一个快照 rbd snap rm {pool-name}/{image-name}@{snap-name} rbd snap rm rbd/foo@snapname 清除所有快照 rbd snap purge {pool-name}/{image-name} rbd snap purge rbd/foo 列出快照 Reference [1] 中国铁路：Ceph在单集群1551个OSD下的挑战\n[2] rbd – manage rados block device (RBD) images\n[3] pgcalc\n","wordCount":"465","inLanguage":"zh","datePublished":"2019-09-30T00:00:00Z","dateModified":"2023-09-06T23:10:36+08:00","author":{"@type":"Person","name":"cylon"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.oomkill.com/2019/09/03-1-acquaintance-rdb/"},"publisher":{"@type":"Organization","name":"Cylon's Collection","logo":{"@type":"ImageObject","url":"https://www.oomkill.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.oomkill.com/><img src=https://www.oomkill.com/favicon.ico alt aria-label=logo height=20>Cylon's Collection</a><div class=logo-switches><button id=theme-toggle><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.oomkill.com/archives><span>归档</span></a></li><li><a href=https://www.oomkill.com/tags><span>标签</span></a></li><li><a href=https://www.oomkill.com/search><span>搜索</span></a></li><li><a href=https://www.oomkill.com/about accesskey=/><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Ceph RBD - 初识块存储RBD</h1><div class=post-meta><span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>2019-09-30</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg><span>465 字</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><span>3 分钟</span></span>
<span class=pe-post-meta-item>&nbsp;·&nbsp;<svg t="1714036239378" fill="currentcolor" class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6659" width="256" height="256"><path d="M690 78.2c-18.6-18.8-49-19-67.8-.4s-19 49-.4 67.8l255.4 258.6c67.8 68.6 67.8 178.8.0 247.4L653.4 878.2c-18.6 18.8-18.4 49.2.4 67.8s49.2 18.4 67.8-.4l224-226.4c104.8-106 104.8-276.4.0-382.4L690 78.2zM485.4 101.4c-24-24-56.6-37.4-90.6-37.4H96C43 64 0 107 0 160v299c0 34 13.4 66.6 37.4 90.6l336 336c50 50 131 50 181 0l267-267c50-50 50-131 0-181l-336-336zM96 160h299c8.4.0 16.6 3.4 22.6 9.4l336 336c12.4 12.4 12.4 32.8.0 45.2l-267 267c-12.4 12.4-32.8 12.4-45.2.0l-336-336c-6-6-9.4-14.2-9.4-22.6V160zm192 128a64 64 0 10-128 0 64 64 0 10128 0z" p-id="6660"/></svg></span><ul class=pe-post-meta-item><a href=https://www.oomkill.com/tags/storage/>#Storage</a></ul></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details><summary><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e4%bb%80%e4%b9%88%e6%98%af%e5%9d%97%e5%ad%98%e5%82%a8-rbd aria-label="什么是块存储 RBD">什么是块存储 RBD</a><ul><li><a href=#%e5%a6%82%e4%bd%95%e7%a1%ae%e5%ae%9a-ceph-mon-%e7%9a%84%e6%95%b0%e9%87%8f aria-label="如何确定 ceph-mon 的数量">如何确定 ceph-mon 的数量</a></ul><li><a href=#rbd-%e5%ad%98%e5%82%a8%e5%8d%95%e5%85%83 aria-label="RBD 存储单元">RBD 存储单元</a><ul><li><a href=#pool aria-label=POOL>POOL</a></ul><li><a href=#librdb-%e5%92%8c-librados aria-label="librdb 和 librados">librdb 和 librados</a><li><a href=#rbb-%e6%94%af%e6%8c%81%e7%9a%84%e5%8a%9f%e8%83%bd aria-label="RBB 支持的功能">RBB 支持的功能</a><ul><li><a href=#%e5%bf%ab%e7%85%a7 aria-label=快照>快照</a><li><a href=#%e5%88%97%e5%87%ba%e5%bf%ab%e7%85%a7 aria-label=列出快照>列出快照</a></ul><li><a href=#reference aria-label=Reference>Reference</a></li></div></details></div></aside><script src=/js/pe-toc.min.445eb1bfc5e85dd13b9519fcc2a806522e9629b6224a2974052789ba00ab78af.js integrity="sha256-RF6xv8XoXdE7lRn8wqgGUi6WKbYiSil0BSeJugCreK8="></script><div class=post-content><h2 id=什么是块存储-rbd>什么是块存储 RBD<a hidden class=anchor aria-hidden=true href=#什么是块存储-rbd>#</a></h2><p>Ceph RBD (<strong>R</strong>ADOS <strong>B</strong>lock <strong>D</strong>evice) 是 Ceph 提供的三种存储类型之一 (块存储 RBD, 文件存储 CephFS, 对象存储 RGW)，也是另外两个存储类型 (文件存储 CephFS, 对象存储 RGW) 的底座，位于 RADOS 架构中的最底层，由下图可以看出</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/Screenshot-from-2015-10-28-130109.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/Screenshot-from-2015-10-28-130109.png#center alt onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><center>图：Ceph RADOS架构图</center><center><em>Source：</em>https://www.supportsages.com/ceph-part-3-technical-architecture-and-components/</center><br><p>RADOS 是可信赖的自动分布式对象存储 (<strong>R</strong>eliable <strong>A</strong>utonomous <strong>D</strong>istributed <strong>O</strong>bject <strong>S</strong>tore) 的简写，通俗来说，RADOS 代表的就是整个 Ceph 集群，数据对象在集群中的存储方式会“将对象复制为多副本” 以实现容错，所以 Ceph 集群的底座就是 RADOS，一个 RADOS 集群的组件通常包含三个，<em>OSD Daemon</em> , <em>MDS</em>, <em>MON</em></p><ul><li><strong>Object Storage Device</strong> (<em>OSD</em>) Daemon：RADOS集群中负责存储守护进程，与 OSD (数据的物理或逻辑存储单元【通常指一个硬盘】)交互。集群中的每个 Ceph Node 都必须运行 OSD Daemon。对于每个 OSD，可以有一个关联的硬盘 (通常一个OSD Daemon 对应一个存储单元)。</li><li><strong>MONITORS</strong> (Mon Daemon)：Monitor (ceph-mon) 不是集群存储组件的一部分，但它通过监视 OSD 状态并生成 “Cluster Map” 而成为 RADOS 不可或缺的一部分。它监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。一般来说，它充当存储集群中所有 OSD 的 Monitor</li><li><strong>Manager</strong> (MGR Daemon)：Manager (ceph-mgr) 是与 ceph-mon 一同运行的守护进程，为外部监控和管理系统提供额外的监视和接口。默认情况下，ceph-mgr 除了确保其正在运行之外不需要其他配置。如果没有运行 ceph-mgr，<code>ceph -s</code> 将会看到一条 WARN；不管是使用什么方式部署的集群 ( ceph-deploy, cephadm)，ceph-mgr 总会 与 ceph-mon 同时运行在一个节点上，也可单独运行在 Ceph Node 之上。</li></ul><blockquote><p>通常 Monitor (ceph-mon) 不构成“<strong>存储</strong>”集群的一部分，只是通过监视 OSD 状态并生成 Cluster map 而成为 ceph存储集群中不可缺少的组件。它通过监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。</p></blockquote><h3 id=如何确定-ceph-mon-的数量>如何确定 ceph-mon 的数量<a hidden class=anchor aria-hidden=true href=#如何确定-ceph-mon-的数量>#</a></h3><p>ceph-mon 的数量最好是奇数，并且数量个数时有限限制的，这里在总结一下 ceph monitor 的作用：</p><ul><li>Monitor 不向客户端提供存储的对象，由于它只是一个监控节点，因此它不保存/管理任何对象，也不属于对象存储的一部分，但它仍然是 RADOS 的一部分，但不用于数据/对象存储。</li><li>它维护簇映射状态，即：存储集群客户端/应用程序从 ceph-mon 检索集群映射的副本</li><li>通常很奇怪并且数量有限，因为他们的工作相当简单，维护 OSD 的状态。</li><li>为分布式决策提供共识，当要针对 OSD 的状态做出特定决策时，它提供了一般规则。 当多个 OSD 要进行对等互连或复制时，监视器会自行决定如何进行对等互连，而不是由 ODS 自行决定。</li><li>Ceph OSD 守护进程检查自身状态和其他 OSD 的状态并向监视器报告。</li></ul><p>中国铁路2020年分享的 1550+ OSD PB 级别存储 ceph-con 数量从 “3” 升级到 “5” 后，趋势稳定 <sup><a href=#1>[1]</a></sup></p><h2 id=rbd-存储单元>RBD 存储单元<a hidden class=anchor aria-hidden=true href=#rbd-存储单元>#</a></h2><p>RBD 块设备 (RADOS Block Device) 在 Ceph 中被称为 image。image 由 “元数据” 和 ”数据“两部分组成，其中元数据存储在多个特殊的 RADOS 对象中，而数据被自动”条带化“成多个 RADOS 对象进行存储。除了 image 自身的元数据之外，在 image 所属的 存储池 (Pool) 中都还有一组特殊的 RADOS 对象记录 image 关联关系或附加信息等相关的 RBD 管理元数据。所有的数据对象和元数据对象都依据 CRUSH 规则 (CRUSH RULE) 存储在底层的 OSD 设备上，因此 RBD 块设备自动继承了 RADOS 对象的数据冗余保护机制和一致性策略。</p><p>对于 RBD Image 官方是这么描述的 <sup><a href=#2>[2]</a></sup></p><blockquote><p>RBD images are simple block devices that are <strong>striped</strong> over objects and stored in a RADOS object store. The size of the objects the image is striped over must be a power of two.</p></blockquote><p>由此，我们可以得知，”镜像“ (RBD IMAGE) 的表现就是一个块设备</p><h3 id=pool>POOL<a hidden class=anchor aria-hidden=true href=#pool>#</a></h3><p>存储池是用于存储对象的逻辑分区，一个存储池中可以放置多个”IMAGE“。在官方给出的，存储池提供了如下特性:</p><p>池提供以下功能：</p><ol><li><p>冗余性 (<strong>Resilience</strong>)：可以设置允许失败的 OSD 数量，而不会丢失任何数据。如果您的集群使用 ”复制池“ (replicated )，那么可以容忍的 OSD 故障数量等于复制的数量。</p><p>例如：典型配置存储一个对象及其每个 RADOS 对象的两个副本（即size=3，默认值），但您可以根据池的需求配置副本数量。对于纠删码池 (erasure-coded)，冗余性定义为编码块的数量（例如，默认纠删码配置文件中的m = 2）。</p></li><li><p>放置组（Placement Groups，PGs）：您可以为池设置放置组（PGs）的数量。在典型配置中，每个 OSD 的目标 PG 数量约为 100 个 PGs。这提供了合理的负载均衡，而不会消耗过多的计算资源。在设置多个存储池时，请小心为每个存储池和整个集群设置合适数量的 PGs。每个 PG 都属于特定的存储池：当多个存储池使用相同的 OSDs 时，请确保每个 OSD 上的 PG 副本总数在所需的 “每个 OSD 上 PG数量的目标范围内”。要计算适合您的池的PG数量，请使用 pgcalc <sup><a href=#3>[3]</a></sup> 工具。</p></li><li><p>CRUSH规则：当数据存储在池中时，对象及其副本（或在纠删码池中的块）在您的集群中的放置由 CRUSH 规则管理。如果默认规则不适合您的用例，可以为池创建自定义 CRUSH 规则。</p></li><li><p>快照：命令 <code>ceph osd pool mksnap</code> 可创建基于存储池的快照。</p></li></ol><h2 id=librdb-和-librados>librdb 和 librados<a hidden class=anchor aria-hidden=true href=#librdb-和-librados>#</a></h2><p>要想使用块设备，就涉及到 RADOS 之上数据的交互，我们已经了解到了 RBD 是为 KVM 等虑拟化技术和云 OS（如 OpenStack 和 CloudStack）提供高性能和无限可扩展性的存储后端，这些系统依赖于 libvirt 和 QEMU 实用程序与 RBD 进行集成。</p><p>在Ceph中，提供了一种 librados 的库，它可以使得 客户端 与 Ceph 集群间的交互，在 RADOS 之上 存在一个 &ldquo;librados"库，而 &ldquo;librbd&rdquo; 库则是构建与 “librados” 之上的的库，提供了块存储的功能，下面是两个库在“用途”和“功能”上有一些重要区别：</p><ol><li><strong>librados</strong>（RADOS客户端库）：<ul><li><strong>用途</strong>：librados是用于与Ceph的底层RADOS存储层交互的库。它提供了与Ceph集群中的对象存储进行直接通信的API，允许应用程序执行各种存储操作，如读取、写入、删除和管理存储对象。</li><li><strong>功能</strong>：librados允许应用程序直接访问Ceph集群，而不需要高级抽象，这意味着应用程序可以更精细地控制数据的读写和存储策略。librados可以用于构建自定义数据存储和管理解决方案。</li></ul></li><li><strong>librbd</strong>（RBD客户端库）：<ul><li><strong>用途</strong>：librbd是用于与Ceph中的RBD（Rados Block Device）存储层交互的库。它构建在librados之上，提供了更高级别的抽象，用于处理块设备（镜像）操作，如创建、映射、快照和克隆等。</li><li><strong>功能</strong>：librbd简化了块设备操作，并提供了易于使用的API，使应用程序能够轻松地管理RBD镜像。它是构建虚拟化平台、云存储、容器存储等应用的关键组件。</li></ul></li></ol><p>要使用 RBD 存储池需要先启用，而却需要注意的是 rbd 存储池并不能直接用于块设备，而是需要事先在其中按需创建映像（image），而 “映像” 代表了真正的块设备</p><h2 id=rbb-支持的功能>RBB 支持的功能<a hidden class=anchor aria-hidden=true href=#rbb-支持的功能>#</a></h2><ul><li>快照</li><li>排他锁/独占锁</li><li>镜像</li><li>实时迁移</li><li>..</li></ul><h3 id=快照>快照<a hidden class=anchor aria-hidden=true href=#快照>#</a></h3><p>RBD 快照 (<em><strong>snapshot</strong></em>) 是 image 在某个特定时间点的只读逻辑副本，类似于虚拟机的快照，快照也是 RBD 的高级功能之一，使得可以用户创建映像快照以保留时间点状态历史记录。 Ceph还支持快照分层，使您可以快速轻松地克隆镜像（例如虚拟机镜像）。 Ceph RBD 快照的使用通过命令 rbd 和几个更高级的接口进行管理，包括 QEMU, libvirt, OpenStack 和 CloudStack。</p><p>由于 Ceph RBD 不知道 “镜像” 内的任何文件系统，因此快照仅是崩溃一致的，除非它们在挂载(<em>mouting</em>) 操作系统内进行协调。因此，我们建议您在拍摄快照之前暂停或停止 I/O。</p><blockquote><p>如果镜像包含文件系统，则在拍摄快照之前文件系统应处于内部一致状态。即需要停止IO</p></blockquote><p>快照的操作</p><p>ceph 有专门的命令 <code>rbd</code> 可以进行 RBD 相关的操作，例如快照的命令为 <code>rbd snapshot</code></p><table><thead><tr><th>命令</th><th>语法</th><th>样例</th></tr></thead><tbody><tr><td>创建快照</td><td>rbd snap create {pool-name}/{image-name}@{snap-name}</td><td>rbd snap create rbd/foo@snapname</td></tr><tr><td>列出快照</td><td>rbd snap ls {pool-name}/{image-name}</td><td>rbd snap ls rbd/foo</td></tr><tr><td>回滚快照</td><td>rbd snap rollback {pool-name}/{image-name}@{snap-name}</td><td>rbd snap rollback rbd/foo@snapname</td></tr><tr><td>删除一个快照</td><td>rbd snap rm {pool-name}/{image-name}@{snap-name}</td><td>rbd snap rm rbd/foo@snapname</td></tr><tr><td>清除所有快照</td><td>rbd snap purge {pool-name}/{image-name}</td><td>rbd snap purge rbd/foo</td></tr></tbody></table><h3 id=列出快照>列出快照<a hidden class=anchor aria-hidden=true href=#列出快照>#</a></h3><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><blockquote><p><sup id=1>[1]</sup> <a href=https://www.bilibili.com/video/BV1oe411W7Jj target=_blank rel="noopener nofollow noreferrer"><em><strong>中国铁路：Ceph在单集群1551个OSD下的挑战</strong></em></a></p><p><sup id=2>[2]</sup> <a href=https://docs.ceph.com/en/latest/man/8/rbd/#description target=_blank rel="noopener nofollow noreferrer"><em><strong>rbd &ndash; manage rados block device (RBD) images</strong></em></a></p><p><sup id=3>[3]</sup> <a href=https://old.ceph.com/pgcalc/ target=_blank rel="noopener nofollow noreferrer"><em><strong>pgcalc</strong></em></a></p></blockquote></div><div class=pe-copyright><hr><blockquote><p>本文为原创内容，版权归作者所有。如需转载，请在文章中声明本文标题及链接。</p><p>文章标题：Ceph RBD - 初识块存储RBD</p><p>文章链接：<a href=https://www.oomkill.com/2019/09/03-1-acquaintance-rdb/ target=_blank>https://www.oomkill.com/2019/09/03-1-acquaintance-rdb/</a></p><p>许可协议：<a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></p></blockquote></div><div class=comments-separator></div><h3 class=relatedContentTitle>相关阅读</h3><ul class=relatedContent><li><a href=/2019/07/03-2-rbd-management/><span>Ceph RBD - 关于RBD的操作与管理</span></a></li><li><a href=/2019/07/05-1-rgw/><span>Ceph对象存储概述</span></a></li><li><a href=/2019/07/04-1-cephfs/><span>Ceph文件系统概述</span></a></li><li><a href=/2019/06/07-1-cephx/><span>Ceph安全 - CephX</span></a></li><li><a href=/2019/06/01-1-ceph-acquaintance/><span>Ceph概念 - 初识Ceph</span></a></li></ul><div class=comments-separator></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.oomkill.com/tags/storage/>Storage</a></li></ul><nav class=paginav><a class=prev href=https://www.oomkill.com/2019/10/go-regular-expression/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></polyline></svg>&nbsp;</span>
<span>正则表达式在go中使用</span>
</a><a class=next href=https://www.oomkill.com/2019/09/ch10-linux-with-ldap/><span class=title></span>
<span>理解ldap应用 - Linux系统接入OpenLDAP做认证后端&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span></a></nav></footer><div class=pe-comments-decoration><p class=pe-comments-title></p><p class=pe-comments-subtitle></p></div><div id=pe-comments></div><script src=/js/pe-go-comment.min.86a214102576ba5f9b7bdc29eed8d58dd56e34aef80b3c65c73ea9cc88443696.js integrity="sha256-hqIUECV2ul+be9wp7tjVjdVuNK74Czxlxz6pzIhENpY="></script><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="dark"?"dark":"light",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"cylonchau/cylonchau.github.io","data-repo-id":"R_kgDOIRlNSQ","data-category":"Announcements","data-category-id":"DIC_kwDOIRlNSc4CXy1U","data-mapping":"pathname","data-term":"posts/03-1 Acquaintance-RDB","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":getStoredTheme(),"data-lang":"zh-TW","data-loading":"lazy",crossorigin:"anonymous",async:""},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#pe-comments").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2024 <a href=https://www.oomkill.com/>Cylon's Collection</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> on
<a href=https://pages.github.com/ rel=noopener target=_blank>GitHub Pages</a> & Theme
        <a href=https://github.com/tofuwine/PaperMod-PE rel=noopener target=_blank>PaperMod-PE</a></span></footer><div class=pe-right-sidebar><a href=javascript:void(0); id=theme-toggle-float class=pe-float-btn><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</a><a href=#top class=pe-float-btn id=top-link><span id=pe-read-progress></span></a></div><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>