<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>逻辑回归 | Cylon's Collection</title><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NP3JNCPR" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><meta name=keywords content="MachineLearning,algorithm,CS"><meta name=description content="Overview 逻辑回归通常用于分类算法，例如预测某事是 true 还是 false（二元分类）。例如，对电子邮件进行分类，该算法将使用电子邮件中的单词作为特征，并据此预测电子邮件是否为垃圾邮件。用数学来讲就是指，假设因变量是 Y，而自变量集是 X，那么逻辑回归将预测因变量 $P(Y=1)$ 作为自变量集 X 的函数。
逻辑回归性能在线性分类中是最好的，其核心为基于样本属于某个类别的概率。这里的概率必须是连续的并且在 (0, 1) 之间（有界）。它依赖于阈值函数来做出称为 Sigmoid 或 Logistic 函数决定的。
学好逻辑回归，需要了解逻辑回归的概念、优势比 (OR) 、Logit 函数、Sigmoid 函数、 Logistic 函数及交叉熵或Log Loss
Prerequisite odds ratio explain odds ratio是预测变量的影响。优势比取决于预测变量是分类变量还是连续变量。
连续预测变量：$OR > 1$ 表示，随着预测变量的增加，事件发生的可能性增加。$OR < 1$ 表示随着预测变量的增加，事件发生的可能性较小。 分类预测变量：事件发生在预测变量的 2 个不同级别的几率；如 A,B，$OR > 1$ 表示事件在 A 级别的可能性更大。$OR<1$ 表示事件更低的可能是在A。 例如，假设 X 是受影响的概率，Y 是不受影响的概率，则 $OR= \frac{X}{Y}$ ，那么 $OR = \frac{P}{(1-P)}$ ，P是事件的概率。
让概率的范围为 [0,1] ，假设 $P(success)=0.8$ ，$Q(failure) = 0.2$ ；$OR$ 则是 成功概率和失败概率的比值，如：$O(success)=\frac{P}{Q} = \frac{0."><meta name=author content="cylon"><link rel=canonical href=https://www.oomkill.com/2022/06/logistic-regression/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://www.oomkill.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.oomkill.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.oomkill.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.oomkill.com/favicon.ico><link rel=mask-icon href=https://www.oomkill.com/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://www.oomkill.com/2022/06/logistic-regression/><noscript><style>#theme-toggle,#top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=/assets/css/pe.min.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/pe.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/v4-shims.min.css><script defer src=https://cdn.staticfile.net/jquery/3.5.1/jquery.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/fancybox/3.5.7/jquery.fancybox.min.css><script defer src=https://cdn.staticfile.net/fancybox/3.5.7/jquery.fancybox.min.js></script><script id=MathJax-script async src=https://cdn.staticfile.net/mathjax/3.2.2/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["\\$","\\$"]]}}</script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script>(function(e,t,n,s,o){e[s]=e[s]||[],e[s].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var a=t.getElementsByTagName(n)[0],i=t.createElement(n),r=s!="dataLayer"?"&l="+s:"";i.async=!0,i.src="https://www.googletagmanager.com/gtm.js?id="+o+r,a.parentNode.insertBefore(i,a)})(window,document,"script","dataLayer","GTM-NP3JNCPR")</script><meta property="og:title" content="逻辑回归"><meta property="og:description" content="Overview 逻辑回归通常用于分类算法，例如预测某事是 true 还是 false（二元分类）。例如，对电子邮件进行分类，该算法将使用电子邮件中的单词作为特征，并据此预测电子邮件是否为垃圾邮件。用数学来讲就是指，假设因变量是 Y，而自变量集是 X，那么逻辑回归将预测因变量 $P(Y=1)$ 作为自变量集 X 的函数。
逻辑回归性能在线性分类中是最好的，其核心为基于样本属于某个类别的概率。这里的概率必须是连续的并且在 (0, 1) 之间（有界）。它依赖于阈值函数来做出称为 Sigmoid 或 Logistic 函数决定的。
学好逻辑回归，需要了解逻辑回归的概念、优势比 (OR) 、Logit 函数、Sigmoid 函数、 Logistic 函数及交叉熵或Log Loss
Prerequisite odds ratio explain odds ratio是预测变量的影响。优势比取决于预测变量是分类变量还是连续变量。
连续预测变量：$OR > 1$ 表示，随着预测变量的增加，事件发生的可能性增加。$OR < 1$ 表示随着预测变量的增加，事件发生的可能性较小。 分类预测变量：事件发生在预测变量的 2 个不同级别的几率；如 A,B，$OR > 1$ 表示事件在 A 级别的可能性更大。$OR<1$ 表示事件更低的可能是在A。 例如，假设 X 是受影响的概率，Y 是不受影响的概率，则 $OR= \frac{X}{Y}$ ，那么 $OR = \frac{P}{(1-P)}$ ，P是事件的概率。
让概率的范围为 [0,1] ，假设 $P(success)=0.8$ ，$Q(failure) = 0.2$ ；$OR$ 则是 成功概率和失败概率的比值，如：$O(success)=\frac{P}{Q} = \frac{0."><meta property="og:type" content="article"><meta property="og:url" content="https://www.oomkill.com/2022/06/logistic-regression/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-22T23:00:36+08:00"><meta property="og:site_name" content="Cylon's Collection"><meta name=twitter:card content="summary"><meta name=twitter:title content="逻辑回归"><meta name=twitter:description content="Overview 逻辑回归通常用于分类算法，例如预测某事是 true 还是 false（二元分类）。例如，对电子邮件进行分类，该算法将使用电子邮件中的单词作为特征，并据此预测电子邮件是否为垃圾邮件。用数学来讲就是指，假设因变量是 Y，而自变量集是 X，那么逻辑回归将预测因变量 $P(Y=1)$ 作为自变量集 X 的函数。
逻辑回归性能在线性分类中是最好的，其核心为基于样本属于某个类别的概率。这里的概率必须是连续的并且在 (0, 1) 之间（有界）。它依赖于阈值函数来做出称为 Sigmoid 或 Logistic 函数决定的。
学好逻辑回归，需要了解逻辑回归的概念、优势比 (OR) 、Logit 函数、Sigmoid 函数、 Logistic 函数及交叉熵或Log Loss
Prerequisite odds ratio explain odds ratio是预测变量的影响。优势比取决于预测变量是分类变量还是连续变量。
连续预测变量：$OR > 1$ 表示，随着预测变量的增加，事件发生的可能性增加。$OR < 1$ 表示随着预测变量的增加，事件发生的可能性较小。 分类预测变量：事件发生在预测变量的 2 个不同级别的几率；如 A,B，$OR > 1$ 表示事件在 A 级别的可能性更大。$OR<1$ 表示事件更低的可能是在A。 例如，假设 X 是受影响的概率，Y 是不受影响的概率，则 $OR= \frac{X}{Y}$ ，那么 $OR = \frac{P}{(1-P)}$ ，P是事件的概率。
让概率的范围为 [0,1] ，假设 $P(success)=0.8$ ，$Q(failure) = 0.2$ ；$OR$ 则是 成功概率和失败概率的比值，如：$O(success)=\frac{P}{Q} = \frac{0."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.oomkill.com/posts/"},{"@type":"ListItem","position":2,"name":"逻辑回归","item":"https://www.oomkill.com/2022/06/logistic-regression/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"逻辑回归","name":"逻辑回归","description":"Overview 逻辑回归通常用于分类算法，例如预测某事是 true 还是 false（二元分类）。例如，对电子邮件进行分类，该算法将使用电子邮件中的单词作为特征，并据此预测电子邮件是否为垃圾邮件。用数学来讲就是指，假设因变量是 Y，而自变量集是 X，那么逻辑回归将预测因变量 $P(Y=1)$ 作为自变量集 X 的函数。\n逻辑回归性能在线性分类中是最好的，其核心为基于样本属于某个类别的概率。这里的概率必须是连续的并且在 (0, 1) 之间（有界）。它依赖于阈值函数来做出称为 Sigmoid 或 Logistic 函数决定的。\n学好逻辑回归，需要了解逻辑回归的概念、优势比 (OR) 、Logit 函数、Sigmoid 函数、 Logistic 函数及交叉熵或Log Loss\nPrerequisite odds ratio explain odds ratio是预测变量的影响。优势比取决于预测变量是分类变量还是连续变量。\n连续预测变量：$OR \u0026gt; 1$ 表示，随着预测变量的增加，事件发生的可能性增加。$OR \u0026lt; 1$ 表示随着预测变量的增加，事件发生的可能性较小。 分类预测变量：事件发生在预测变量的 2 个不同级别的几率；如 A,B，$OR \u0026gt; 1$ 表示事件在 A 级别的可能性更大。$OR\u0026lt;1$ 表示事件更低的可能是在A。 例如，假设 X 是受影响的概率，Y 是不受影响的概率，则 $OR= \\frac{X}{Y}$ ，那么 $OR = \\frac{P}{(1-P)}$ ，P是事件的概率。\n让概率的范围为 [0,1] ，假设 $P(success)=0.8$ ，$Q(failure) = 0.2$ ；$OR$ 则是 成功概率和失败概率的比值，如：$O(success)=\\frac{P}{Q} = \\frac{0.","keywords":["MachineLearning","algorithm","CS"],"articleBody":"Overview 逻辑回归通常用于分类算法，例如预测某事是 true 还是 false（二元分类）。例如，对电子邮件进行分类，该算法将使用电子邮件中的单词作为特征，并据此预测电子邮件是否为垃圾邮件。用数学来讲就是指，假设因变量是 Y，而自变量集是 X，那么逻辑回归将预测因变量 $P(Y=1)$ 作为自变量集 X 的函数。\n逻辑回归性能在线性分类中是最好的，其核心为基于样本属于某个类别的概率。这里的概率必须是连续的并且在 (0, 1) 之间（有界）。它依赖于阈值函数来做出称为 Sigmoid 或 Logistic 函数决定的。\n学好逻辑回归，需要了解逻辑回归的概念、优势比 (OR) 、Logit 函数、Sigmoid 函数、 Logistic 函数及交叉熵或Log Loss\nPrerequisite odds ratio explain odds ratio是预测变量的影响。优势比取决于预测变量是分类变量还是连续变量。\n连续预测变量：$OR \u003e 1$ 表示，随着预测变量的增加，事件发生的可能性增加。$OR \u003c 1$ 表示随着预测变量的增加，事件发生的可能性较小。 分类预测变量：事件发生在预测变量的 2 个不同级别的几率；如 A,B，$OR \u003e 1$ 表示事件在 A 级别的可能性更大。$OR\u003c1$ 表示事件更低的可能是在A。 例如，假设 X 是受影响的概率，Y 是不受影响的概率，则 $OR= \\frac{X}{Y}$ ，那么 $OR = \\frac{P}{(1-P)}$ ，P是事件的概率。\n让概率的范围为 [0,1] ，假设 $P(success)=0.8$ ，$Q(failure) = 0.2$ ；$OR$ 则是 成功概率和失败概率的比值，如：$O(success)=\\frac{P}{Q} = \\frac{0.8}{0.2} = 4$ , $O(failure)=\\frac{Q}{P} = \\frac{0.2}{0.8} = 0.25$ 。\nodds和probability 的区别 probability 表示在多次实验中，看到改事件的几率，位于 [0,1] 之间\nodds 表示 $\\frac{(事件发生的概率)}{(事件不会发生的概率)}$ 的比率，位于 [0,∞]\n例如赛马，一匹马跑 100 场比赛，赢了 80 场，那么获胜的概率是 $\\frac{80}{100} = 0.80 = 80%$ ，获胜的几率是 $\\frac{80}{20}=4:1$\n总结：probability 和 odds 之间的主要区别：\n“odds”用于描述是否有可能发生事件。相反，probability决定了事件发生的可能性，即事件发生的频率。 odds以比例表示，probability以百分比形式或小数表示。 odds通常从 0 ~ ∞ ，其中0定义事件发生的可能性，∞ 表示发生的可能性。相反，probability 介于 0~1之间。因此，probability越接近于0，不发生的可能性就越大，越接近于1，发生的可能性就越高。 Reference The Difference Between “Probability” and “Odds”\n通过示例陈述公式 假设一个体校的录取率中，10 个男生中有 7 个被录取，而10 个女生中有3个被录取。找出男生被录取的概率？\n那么通过已知条件，设 P 为录取概率，Q则为未被录取的概率，那么\n男生被录取的概率为： $P=\\frac{7}{10} = 0.7$ $Q=1-0.7 = 0.3$ 女生被录取的概率为： $P=\\frac{3}{10}=0.3$ $Q=1-0.3=0.7$ 录取优势比： $OR(boy)=\\frac{0.7}{0.3}=2.33$ $OR(Gril) = \\frac{0.3}{0.7}=0.42$ 因此，一个男生被录取的几率为 $OR=\\frac{2.33}{0.42}=5.44$\nLogit 函数 logit函数是Odd Ratio 的对数 logarithm , 给出 0~1 范围内的输入，然后将它们转换为整个实数范围内的值。如：假设P，则 $\\frac{P}{(1-P)}$ 为对应的OR；OR 的 logit 的公式为：$loggit(P) = log(odds) = log(\\frac{P}{1-P})$.\n以一辆汽车是否出售为例，1为出售，0为不出售，则等式 $P_i=B_0+B_1 * (Price_i) + \\epsilon$\n$ln(\\frac{P}{1-P}) = \\beta_0 + \\beta_1X_1+\\beta_2X_2… + \\beta_nX_N$ ,对于简单的逻辑回归，有两个系数：\n$\\beta_0$ 截距 ：X 变量为 0 时的对数 odds ratio $\\beta_1$ 斜率：odds ratio随X增加（或减少），1的变化 例如：假设简单逻辑回归模型是 $Ln(odds) = -5.5 + 1.2*X$ ,那么 $\\beta_0=-5.5$ ，$\\beta_1 = 1.2$ ，意味着，X=0时，$odds\\ ratio = 0$ ，X每增加一个单位 odds ration 增加 1.2（（X 增加2个单位odds ratio增加 2.4….）\n求解\n通过上面的公式实际上不明白这些具体是什么，就可以通过求P来找到有结果的概率与截距 $β_0$ 之间的关系，已知 $n=log_ab$ , $ a^n=b$ ，那么一个简单的逻辑回归公式为 $log(\\frac{P}{1-P}) = \\beta_0+\\beta1X$ ，对这个公式进行推导：\n$\\frac{P}{1-P} = e^{\\beta_0+e^\\beta1*X}$ $P = e^{\\beta_0+e^\\beta1X} - Pe^{\\beta_0+e^\\beta1X}$ $P(1+e^{\\beta_0+e^\\beta1X}) = e^{\\beta_0+e^\\beta1X}$ $P=\\frac{e^{\\beta_0+e^\\beta1X}}{1+e^{\\beta_0+e^\\beta1X}}$ 当 $X=0$ ,则 $\\beta_1*X$ 没意义，公式为：$P = frac{e^{β_0}}{(1+e^{β_0})}$ ，其中e是一个常数，python为 math.e\n如果单纯不算概率，只看截距符号，那么满足：\n如果截距为负号：则产生结果的概率将 \u003c 0.5。 如果截距为正号：那么产生结果的概率将 \u003e 0.5。 如果截距等于 0：那么得到结果的概率正好是 0.5。 通过例子来说明这点：假设研究为抽烟对心脏健康的影响，下表显示了一个逻辑回归\nCoefficient Standard Error p-value Intercept -1.93 0.13 \u003c 0.001 Smoking 0.38 0.17 0.03 由表可知，截距为 -1.93，假设smoking系数为0，那么概率带入公式为：$P=\\frac{e^{\\beta_0}}{1+e^{\\beta_0}} = P=\\frac{e^{-1.93}}{1+e^{-1.93}} = 0.126$(math.e ** -1.93)/(1+math.e ** -1.93)\n如果 Smoking是一个连续变量（每年的吸烟量），在这种情况下，Smoking=0 意味着每年使用0公斤烟草的人即不抽烟的人群；那么这个结果就为，不抽烟的人群在未来10年内心脏有问题几率为 0.126。\n再如果是吸烟者应该怎么计算，假设，每年吸烟量为3kg，那么公式为：$P = \\frac{e^{β0 + β_1X}}{(1+e^{β0 + β_1X})}$ ，在这里 X=3，那么 $P=\\frac{e^{\\beta_1+\\beta_2X}}{(1-e^{\\beta_1+\\beta_2X})} = \\frac{e^{-1.93+0.383}}{(1-e^{-1.93+0.383})} = 0.31$ ；即得出，每年3KG烟草消耗量10年后有心脏问题的概率是 31%\ninterpret\nsigmoid logit 函数的逆函数称Sigmoid 函数，sigmoid方程来源于 logit 为：$P=\\frac{e^{log(odds)}}{(1-e^{log(odds)})} = \\frac{1}{e^{-log(odds)+1}} = \\frac{1}{1+e^{-z}}$ 。\n在python中，np.exp 是求 是求 $e^{x}$ 的值的函数。正好可以用在sigmod函数中，那么sigmoid可以写为\npython 1 2 def sigmoid(z): return 1 / (1 + np.exp(-z)) 交叉熵或对数损失 交叉熵 Cross-Entropy，通常用于量化两个概率分布之间的差异。用于逻辑回归，公式为：$H=\\sum^{x=n}(P(x) \\times log(q(x))$\nMaximum Likelihood Estimation 最大似然估计，Maximum Likelihood Estimation MLE，是概率估算的一种解决方案。MLE在其中寻找一组参数，这些参数将影响数据样本 X 的联合概率的最佳拟合。\n首先，定义一个称为 $\\theta$ theta 的参数，该参数定义概率密度函数的选择和该分布的参数。它可能是一个数值向量，其值平滑变化并映射到不同的概率分布及其参数。在最大似然估计中，我们希望在给定特定概率分布及其参数的最大化情况下从联合概率分布中观察数据的概率，形式上表示为：$P(X|\\theta)$ ，在这种情况下，条件概率通常使用分号 ; 而不是竖线 | ，因为 $\\theta$ 不是随机变量，而是未知参数。表达为 $P(X;\\theta)$ ,或 $P(x_1,x_2,\\ …\\ x_n;\\theta)$ 。\n这样产生的条件概率被称为在给定模型参数 （$\\theta$）的情况下观察变量 $X$ 的概率，并使用符号 L 来 表示似然函数。例如：$L(X;\\theta)$。而最大似然估计的目标是找到使似然函数最大化的一组参数 ( $\\theta$ )，例如产生最大似然值，如：$max(L(X;\\theta))$\n鉴于上述提到的变量 $X$ 是由n个样本组成，可以将其定义为在给定概率分布参数 $\\theta$ 的情况下，变量 $X$ 的联合概率,如这里数据样本为 $x_1,x_2,\\ …\\ ,x_n$ 的联合概率，同时表示为 $L(x_1,x2,\\ …\\ ,x_n;\\theta)$\n大多数情况下，求解似然方程很复杂。会使用对数似然作为一种解决方案。由于对数函数是单调递增的，因此对数似然和似然中的最优参数是相同的。因此定义条件最大似然估计为：$log(P(x_i ; h))$。\n用逻辑回归模型替换h，需要假设一个概率分布。在逻辑回归的情况下，假设数据样本为二项式概率分布，其中每个示例都是二项式的一个结果。伯努利分布只有一个参数：成功结果的概率 P，那么为：\n$P(y=1)=P$ $P(y=0)=1-P$ 那么这个平均值为：$P(y=1)*1+P(y=0)0$，给出P的值公式可以转换为：$P1+(1-p)*0$；这种公式看似没有意义，那么通过一个小例子来了解下\npython 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 二项式似然函数 def likelihood(y, p): return p * y + (1 - p) * (1 - y) # test for y=1 y, p = 1, 0.9 print('y=%.1f, p=%.1f, likelihood: %.3f' % (y, p, likelihood(y, p))) y, yhat = 1, 0.1 print('y=%.1f, p=%.1f, likelihood: %.3f' % (y, p, likelihood(y, p))) # test for y=0 y, yhat = 0, 0.1 print('y=%.1f, p=%.1f, likelihood: %.3f' % (y, p, likelihood(y, p))) y, yhat = 0, 0.9 print('y=%.1f, p=%.1f, likelihood: %.3f' % (y, p, likelihood(y, p))) # y=1.0, p=0.9, likelihood: 0.900 # y=1.0, p=0.9, likelihood: 0.900 # y=0.0, p=0.9, likelihood: 0.100 # y=0.0, p=0.9, likelihood: 0.100 运行示例会为每个案例打印类别y 和预测概率p，其中每个案例的概率是否接近；这里也可以使用对数更新似然函数，$log(p) * y + log(1 – p) * (1 – y)$；最后可以根据数据集中实例求最大似然和最小似然\n$\\sum^{i=1}_n log(p_i) * y_i + log(1 – p_i) * (1 – y_i)$ 最小似然使用反转函数，使负对数自然作为最小似然。上面的公式前加 - 对于计算二项式分布的对数似然相当于计算二项式分布[交叉熵，其中P(class)表示第 class 项概率，q() 表示概率分布，$-(log(q(class0)) \\times P(class0) + log(q(class1)) * P(class1))$\nLR算法实例 在研究如何从数据中估计模型的参数之前，我们需要了解逻辑回归准确计算的内容。\n模型的线性部分（输入的加权和）计算成功事件的log-odds。\nodds ratio：$\\beta_0+\\beta_1 \\times x_1 + \\beta_2 \\times x_2\\ …\\ \\beta_n \\times x_n$ 该模型估计了每个级别的输入变量的log-odds。\n由上面信息了解到，几率 probability 是输赢的比率 如 1:10 ；probability 可以转换为 odds ratio 即成功概率除以不成功概率：$or=\\frac{P}{1-P}$ ；计算or的对数，被称为log-odds是一种度量单位：$log(\\frac{P}{1-P})$，而所求的即为 log-odds的逆函数，而在python中 log 函数是对数，求log的逆方法即 exp 返回n的x次方就是log的逆函数。\n到这里已经和逻辑回归模型很接近了，对数函数公式可以简化为，$P=\\frac{e^{log(odds)}}{(1-e^{log(odds)})}$ ，以上阐述了如何从log-odds转化为odds，然后在到逻辑回归模型。下面通过Python 中的示例来具体计算 probability 、odds 和 log-odds 之间的转换。假设将成功概率定义为 80% 或 0.8，然后将其转换为odds，然后再次转换为概率。\npython 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from math import log from math import exp prob = 0.8 print('Probability %.1f' % prob) # 将 probability 转换为 odds odds = prob / (1 - prob) print('Odds %.1f' % odds) # 将 odds 转换为 log-odds logodds = log(odds) print('Log-Odds %.1f' % logodds) # 转换 log-odds 为 probability prob = 1 / (1 + exp(-logodds)) print('Probability %.1f' % prob) # Probability 0.8 # Odds 4.0 # Log-Odds 1.4 # Probability 0.8 通过这个例子，可以看到odds被转换成大约 1.4 的log-odds，然后正确地转回 0.8 的成功概率。\n逻辑回归实现 首先将实现分为3个步骤：\n预测 评估系数 真实数据集预测 预测 编写一个预测函数，在评估随机梯度下降中的候选系数值时以及在模型最终确定测试数据或新数据进行预测时。\n下面是预测**predict()**函数，它预测给定一组系数的行的输出值。第一个系数是截距，也称为偏差或 b0，它是独立的，不负责输入值。\npython 1 2 3 4 5 def predict(row, coefficients): p = coefficients[0] for i in range(len(row)-1): yhat += coefficients[i + 1] * row[i] return 1.0 / (1.0 + exp(-p)) 准备一些测试数据，Y代表真实的类别\npython 1 2 3 4 5 6 7 8 9 10 11 X1\tX2\tY 2.7810836 2.550537003\t0 1.465489372\t2.362125076\t0 3.396561688\t4.400293529\t0 1.38807019\t1.850220317\t0 3.06407232\t3.005305973\t0 7.627531214\t2.759262235\t1 5.332441248\t2.088626775\t1 6.922596716\t1.77106367\t1 8.675418651\t-0.242068655\t1 7.673756466\t3.508563011\t1 这里有两个输入值，和三个系数，系数是自定义的固定值，那么预测的公式就为\npython 1 2 3 4 5 # 系数为 coef = [-0.406605464, 0.852573316, -1.104746259] y = 1.0 / (1.0 + e^(-(b0 + b1 * X1 + b2 * X2))) # 套入公式（sigma） y = 1.0 / (1.0 + e^(-(-0.406605464 + 0.852573316 * X1 + -1.104746259 * X2))) 完整的代码\npython 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Make a prediction from math import exp # Make a prediction with coefficients def predict(row, coefficients): yhat = coefficients[0] for i in range(len(row)-1): yhat += coefficients[i + 1] * row[i] return 1.0 / (1.0 + exp(-yhat)) # test predictions dataset = [[2.7810836,2.550537003,0], [1.465489372,2.362125076,0], [3.396561688,4.400293529,0], [1.38807019,1.850220317,0], [3.06407232,3.005305973,0], [7.627531214,2.759262235,1], [5.332441248,2.088626775,1], [6.922596716,1.77106367,1], [8.675418651,-0.242068655,1], [7.673756466,3.508563011,1]] coef = [-0.406605464, 0.852573316, -1.104746259] for row in dataset: yhat = predict(row, coef) print(\"Expected=%.3f, Predicted=%.3f [%d]\" % (row[-1], yhat, round(yhat))) 估计系数 这里可以使用我随机梯度下降来估计训练数据的系数值。随机梯度下降需要两个参数：\n学习率 Learning rate：用于限制每个系数每次更新时的修正量。 Epochs：更新系数时遍历训练数据的次数。 在每个epoch更新训练数据中每一行的每个系数。系数会根据模型产生的错误进行更新，误差为预期输出与预测值之间的差异。错误会随着epoch增加而减少\n将每个都加权，并且这些系数以一致的方式进行更新，用公式可以表示为\npython 1 b1(t+1) = b1(t) + learning_rate * (y(t) - p(t)) * p(t) * (1 - p(t)) * x1(t) 那么整合一起为\npython 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 from math import exp # 预测函数 def predict(row, coefficients): p = coefficients[0] for i in range(len(row)-1): p += coefficients[i + 1] * row[i] return 1.0 / (1.0 + exp(-p)) def coefficients_sgd(train, l_rate, n_epoch): coef = [0.0 for i in range(len(train[0]))] # 初始一个系数，第一次为都为0 for epoch in range(n_epoch): sum_error = 0 for row in train: p = predict(row, coef) # 错误为预期值与实际值直接差异 error = row[-1] - p sum_error += error**2 # 截距没有输入变量x，这里为row[0] coef[0] = coef[0] + l_rate * error * p * (1.0 - p) for i in range(len(row)-1): # 其他系数更新 coef[i + 1] = coef[i + 1] + l_rate * error * p * (1.0 - p) * row[i] print('\u003eepoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error)) return coef # Calculate coefficients dataset = [ [2.7810836,2.550537003,0], [1.465489372,2.362125076,0], [3.396561688,4.400293529,0], [1.38807019,1.850220317,0], [3.06407232,3.005305973,0], [7.627531214,2.759262235,1], [5.332441248,2.088626775,1], [6.922596716,1.77106367,1], [8.675418651,-0.242068655,1], [7.673756466,3.508563011,1] ] l_rate = 0.3 n_epoch = 100 coef = coefficients_sgd(dataset, l_rate, n_epoch) print(coef) # \u003eepoch=92, lrate=0.300, error=0.024 # \u003eepoch=93, lrate=0.300, error=0.024 # \u003eepoch=94, lrate=0.300, error=0.024 # \u003eepoch=95, lrate=0.300, error=0.023 # \u003eepoch=96, lrate=0.300, error=0.023 # \u003eepoch=97, lrate=0.300, error=0.023 # \u003eepoch=98, lrate=0.300, error=0.023 # \u003eepoch=99, lrate=0.300, error=0.022 #[-0.8596443546618897, 1.5223825112460005, -2.218700210565016] 这里跟踪了跟踪每个epoch误差平方的总和，以便我们可以在每个epoch中打印出error，实例中使用 0.3 学习率并训练100 个 epoch，每个epoch会打印出其误差平方，最终会打印总系数集\n套用真实数据集 糖尿病数据集 是根据基本的医疗信息，预测印第安人5年内患糖尿病的情况。这是一个二元分类，阴性0与阳性1直接的关系。采用了二项式分布，也可以采用其他分布，如高斯等。\npython 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 from random import seed from random import randrange from csv import reader from math import exp # Load a CSV file def load_csv(filename): dataset = list() with open(filename, 'r') as file: csv_reader = reader(file) for row in csv_reader: if not row: continue dataset.append(row) return dataset # Convert string column to float def str_column_to_float(dataset, column): for row in dataset: row[column] = float(row[column].strip()) # 找到最小和最大的 def dataset_minmax(dataset): minmax = list() for i in range(len(dataset[0])): col_values = [row[i] for row in dataset] value_min = min(col_values) value_max = max(col_values) minmax.append([value_min, value_max]) return minmax # 归一化 def normalize_dataset(dataset, minmax): for row in dataset: for i in range(len(row)): row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0]) # k-folds CV实现 def cross_validation_split(dataset, n_folds): dataset_split = list() dataset_copy = list(dataset) fold_size = int(len(dataset) / n_folds) for i in range(n_folds): fold = list() while len(fold) \u003c fold_size: index = randrange(len(dataset_copy)) fold.append(dataset_copy.pop(index)) dataset_split.append(fold) return dataset_split # 计算准确度百分比 def accuracy_metric(actual, predicted): correct = 0 for i in range(len(actual)): if actual[i] == predicted[i]: correct += 1 return correct / float(len(actual)) * 100.0 # 使用CV评估算法 def evaluate_algorithm(dataset, algorithm, n_folds, *args): folds = cross_validation_split(dataset, n_folds) scores = list() for fold in folds: train_set = list(folds) train_set.remove(fold) train_set = sum(train_set, []) test_set = list() for row in fold: row_copy = list(row) test_set.append(row_copy) row_copy[-1] = None predicted = algorithm(train_set, test_set, *args) actual = [row[-1] for row in fold] accuracy = accuracy_metric(actual, predicted) scores.append(accuracy) return scores # 使用系数进行预测 def predict(row, coefficients): yhat = coefficients[0] for i in range(len(row)-1): yhat += coefficients[i + 1] * row[i] return 1.0 / (1.0 + exp(-yhat)) # 系数生成 def coefficients_sgd(self, train, l_rate, n_epoch): \"\"\" 生成系数 :param train: list, 数据集，可以是训练集 :param l_rate: float, 学习率 :param n_epoch:int，epoch，这里代表进行多少次迭代 :return: None \"\"\" coef = [0.0 for i in range(len(train[0]))] # 初始一个系数，第一次为都为0 for epoch in range(n_epoch): sum_error = 0 for row in train: p = self.predict(row, coef) # 错误为预期值与实际值直接差异 error = row[-1] - p sum_error += error**2 # 截距没有输入变量x，这里为row[0] coef[0] = coef[0] + l_rate * error * p * (1.0 - p) for i in range(len(row)-1): # 其他系数更新 coef[i + 1] = coef[i + 1] + l_rate * error * p * (1.0 - p) * row[i] # print('\u003eepoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error)) return coef # 随机梯度下降的逻辑回归算法 def logistic_regression(self, train, test, l_rate, n_epoch): predictions = list() coef = self.coefficients_sgd(train, l_rate, n_epoch) for row in test: p = self.predict(row, coef) p = round(p) predictions.append(p) return(predictions) seed(1) # 数据预处理 filename = 'pima-indians-diabetes.csv' dataset = load_csv(filename) for i in range(len(dataset[0])): str_column_to_float(dataset, i) # 做归一化 minmax = dataset_minmax(dataset) normalize_dataset(dataset, minmax) # evaluate algorithm n_folds = 5 l_rate = 0.1 n_epoch = 100 scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch) print('Scores: %s' % scores) print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores)))) # 0.35294117647058826 # Scores: [73.8562091503268, 78.43137254901961, 81.69934640522875, 75.81699346405229, 75.81699346405229] # Mean Accuracy: 77.124% 上述是对整个数据集的预测百分比，也可以对对应的类的信息进行输出\nReference Maximum likelihood estimation\nSigmoid Function\nlogistic\nbinary logistic regression\nLR implementation\n","wordCount":"1800","inLanguage":"zh","datePublished":"2022-06-01T00:00:00Z","dateModified":"2023-03-22T23:00:36+08:00","author":{"@type":"Person","name":"cylon"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.oomkill.com/2022/06/logistic-regression/"},"publisher":{"@type":"Organization","name":"Cylon's Collection","logo":{"@type":"ImageObject","url":"https://www.oomkill.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.oomkill.com/><img src=https://www.oomkill.com/favicon.ico alt aria-label=logo height=20>Cylon's Collection</a><div class=logo-switches><button id=theme-toggle><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.oomkill.com/archives><span>归档</span></a></li><li><a href=https://www.oomkill.com/tags><span>标签</span></a></li><li><a href=https://www.oomkill.com/search><span>搜索</span></a></li><li><a href=https://www.oomkill.com/about accesskey=/><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">逻辑回归</h1><div class=post-meta><span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>2022-06-01</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg><span>1800 字</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><span>9 分钟</span></span>
<span class=pe-post-meta-item>&nbsp;·&nbsp;<svg t="1714036239378" fill="currentcolor" class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6659" width="256" height="256"><path d="M690 78.2c-18.6-18.8-49-19-67.8-.4s-19 49-.4 67.8l255.4 258.6c67.8 68.6 67.8 178.8.0 247.4L653.4 878.2c-18.6 18.8-18.4 49.2.4 67.8s49.2 18.4 67.8-.4l224-226.4c104.8-106 104.8-276.4.0-382.4L690 78.2zM485.4 101.4c-24-24-56.6-37.4-90.6-37.4H96C43 64 0 107 0 160v299c0 34 13.4 66.6 37.4 90.6l336 336c50 50 131 50 181 0l267-267c50-50 50-131 0-181l-336-336zM96 160h299c8.4.0 16.6 3.4 22.6 9.4l336 336c12.4 12.4 12.4 32.8.0 45.2l-267 267c-12.4 12.4-32.8 12.4-45.2.0l-336-336c-6-6-9.4-14.2-9.4-22.6V160zm192 128a64 64 0 10-128 0 64 64 0 10128 0z" p-id="6660"/></svg></span><ul class=pe-post-meta-item><a href=https://www.oomkill.com/tags/machinelearning/>#MachineLearning</a>
<a href=https://www.oomkill.com/tags/algorithm/>#Algorithm</a>
<a href=https://www.oomkill.com/tags/cs/>#CS</a></ul></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details><summary><span class=details>目录</span></summary><div class=inner><ul><li><a href=#overview aria-label=Overview>Overview</a><li><a href=#prerequisite aria-label=Prerequisite>Prerequisite</a><ul><li><a href=#odds-ratio aria-label="odds ratio">odds ratio</a><ul><li><a href=#explain aria-label=explain>explain</a><li><a href=#odds%e5%92%8cprobability-%e7%9a%84%e5%8c%ba%e5%88%ab aria-label="odds和probability 的区别">odds和probability 的区别</a></ul></ul><li><a href=#reference aria-label=Reference>Reference</a><ul><ul><li><a href=#%e9%80%9a%e8%bf%87%e7%a4%ba%e4%be%8b%e9%99%88%e8%bf%b0%e5%85%ac%e5%bc%8f aria-label=通过示例陈述公式>通过示例陈述公式</a></ul><li><a href=#logit-%e5%87%bd%e6%95%b0 aria-label="Logit 函数">Logit 函数</a><li><a href=#sigmoid aria-label=sigmoid>sigmoid</a><li><a href=#%e4%ba%a4%e5%8f%89%e7%86%b5%e6%88%96%e5%af%b9%e6%95%b0%e6%8d%9f%e5%a4%b1 aria-label=交叉熵或对数损失>交叉熵或对数损失</a><li><a href=#maximum-likelihood-estimation aria-label="Maximum Likelihood Estimation">Maximum Likelihood Estimation</a></ul><li><a href=#lr%e7%ae%97%e6%b3%95%e5%ae%9e%e4%be%8b aria-label=LR算法实例>LR算法实例</a><li><a href=#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92%e5%ae%9e%e7%8e%b0 aria-label=逻辑回归实现>逻辑回归实现</a><ul><li><a href=#%e9%a2%84%e6%b5%8b aria-label=预测>预测</a><li><a href=#%e4%bc%b0%e8%ae%a1%e7%b3%bb%e6%95%b0 aria-label=估计系数>估计系数</a><li><a href=#%e5%a5%97%e7%94%a8%e7%9c%9f%e5%ae%9e%e6%95%b0%e6%8d%ae%e9%9b%86 aria-label=套用真实数据集>套用真实数据集</a></ul><li><a href=#reference-1 aria-label=Reference>Reference</a></li></div></details></div></aside><script src=/js/pe-toc.min.445eb1bfc5e85dd13b9519fcc2a806522e9629b6224a2974052789ba00ab78af.js integrity="sha256-RF6xv8XoXdE7lRn8wqgGUi6WKbYiSil0BSeJugCreK8="></script><div class=post-content><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>逻辑回归通常用于分类算法，例如预测某事是 <code>true</code> 还是 <code>false</code>（二元分类）。例如，对电子邮件进行分类，该算法将使用电子邮件中的单词作为特征，并据此预测电子邮件是否为垃圾邮件。用数学来讲就是指，假设因变量是 Y，而自变量集是 X，那么逻辑回归将预测因变量 $P(Y=1)$ 作为自变量集 X 的函数。</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/1UgYbimgPXf6XXxMy2yqRLw.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/1UgYbimgPXf6XXxMy2yqRLw.png#center alt=img onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><p>逻辑回归性能在线性分类中是最好的，其核心为基于样本属于某个类别的概率。这里的概率必须是连续的并且在 <code>(0, 1)</code> 之间（有界）。它依赖于阈值函数来做出称为 <code>Sigmoid</code> 或 <code>Logistic</code> 函数决定的。</p><p>学好逻辑回归，需要了解逻辑回归的概念、优势比 (<strong>OR</strong>) 、Logit 函数、Sigmoid 函数、 Logistic 函数及交叉熵或Log Loss</p><h2 id=prerequisite>Prerequisite<a hidden class=anchor aria-hidden=true href=#prerequisite>#</a></h2><h3 id=odds-ratio>odds ratio<a hidden class=anchor aria-hidden=true href=#odds-ratio>#</a></h3><h4 id=explain>explain<a hidden class=anchor aria-hidden=true href=#explain>#</a></h4><p>odds ratio是预测变量的影响。优势比取决于预测变量是分类变量还是连续变量。</p><ul><li>连续预测变量：$OR > 1$ 表示，随着预测变量的增加，事件发生的可能性增加。$OR &lt; 1$ 表示随着预测变量的增加，事件发生的可能性较小。</li><li>分类预测变量：事件发生在预测变量的 2 个不同级别的几率；如 A,B，$OR > 1$ 表示事件在 A 级别的可能性更大。$OR&lt;1$ 表示事件更低的可能是在A。</li></ul><p>例如，假设 X 是受影响的概率，Y 是不受影响的概率，则 $OR= \frac{X}{Y}$ ，那么 $OR = \frac{P}{(1-P)}$ ，P是事件的概率。</p><p>让概率的范围为 <code>[0,1]</code> ，假设 $P(success)=0.8$ ，$Q(failure) = 0.2$ ；$OR$ 则是 成功概率和失败概率的比值，如：$O(success)=\frac{P}{Q} = \frac{0.8}{0.2} = 4$ , $O(failure)=\frac{Q}{P} = \frac{0.2}{0.8} = 0.25$ 。</p><h4 id=odds和probability-的区别>odds和probability 的区别<a hidden class=anchor aria-hidden=true href=#odds和probability-的区别>#</a></h4><ul><li><p><strong>probability</strong> 表示在多次实验中，看到改事件的几率，位于 <code>[0,1]</code> 之间</p></li><li><p><strong>odds</strong> 表示 $\frac{(事件发生的概率)}{(事件不会发生的概率)}$ 的比率，位于 <code>[0,∞]</code></p></li></ul><p>例如赛马，一匹马跑 100 场比赛，赢了 80 场，那么获胜的概率是 $\frac{80}{100} = 0.80 = 80%$ ，获胜的几率是 $\frac{80}{20}=4:1$</p><p><strong>总结</strong>：probability 和 odds 之间的主要区别：</p><ul><li>“odds”用于描述是否有可能发生事件。相反，probability决定了事件发生的可能性，即事件发生的频率。</li><li>odds以比例表示，probability以百分比形式或小数表示。</li><li>odds通常从 <code>0 ~ ∞</code> ，其中0定义事件发生的可能性，<code>∞</code> 表示发生的可能性。相反，probability 介于 <code>0~1</code>之间。因此，probability越接近于0，不发生的可能性就越大，越接近于1，发生的可能性就越高。</li></ul><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><blockquote><p><a href=https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_confidence_intervals/BS704_Confidence_Intervals10.html target=_blank rel="noopener nofollow noreferrer">The Difference Between &ldquo;Probability&rdquo; and &ldquo;Odds&rdquo;</a></p></blockquote><h4 id=通过示例陈述公式>通过示例陈述公式<a hidden class=anchor aria-hidden=true href=#通过示例陈述公式>#</a></h4><p>假设一个体校的录取率中，10 个男生中有 7 个被录取，而10 个女生中有3个被录取。找出男生被录取的概率？</p><p>那么通过已知条件，设 P 为录取概率，Q则为未被录取的概率，那么</p><ul><li>男生被录取的概率为：<ul><li>$P=\frac{7}{10} = 0.7$</li><li>$Q=1-0.7 = 0.3$</li></ul></li><li>女生被录取的概率为：<ul><li>$P=\frac{3}{10}=0.3$</li><li>$Q=1-0.3=0.7$</li></ul></li><li>录取优势比：<ul><li>$OR(boy)=\frac{0.7}{0.3}=2.33$</li><li>$OR(Gril) = \frac{0.3}{0.7}=0.42$</li></ul></li></ul><p>因此，一个男生被录取的几率为 $OR=\frac{2.33}{0.42}=5.44$</p><h3 id=logit-函数>Logit 函数<a hidden class=anchor aria-hidden=true href=#logit-函数>#</a></h3><p>logit函数是<code>Odd Ratio</code> 的对数 <strong>logarithm</strong> , 给出 <code>0~1</code> 范围内的输入，然后将它们转换为整个实数范围内的值。如：假设P，则 $\frac{P}{(1-P)}$ 为对应的OR；OR 的 logit 的公式为：$loggit(P) = log(odds) = log(\frac{P}{1-P})$.</p><p>以一辆汽车是否出售为例，1为出售，0为不出售，则等式 $P_i=B_0+B_1 * (Price_i) + \epsilon$</p><p>$ln(\frac{P}{1-P}) = \beta_0 + \beta_1X_1+\beta_2X_2&mldr; + \beta_nX_N$ ,对于简单的逻辑回归，有两个系数：</p><ul><li>$\beta_0$ 截距 ：X 变量为 0 时的对数 odds ratio</li><li>$\beta_1$ 斜率：odds ratio随X增加（或减少），1的变化</li></ul><p>例如：假设简单逻辑回归模型是 $Ln(odds) = -5.5 + 1.2*X$ ,那么 $\beta_0=-5.5$ ，$\beta_1 = 1.2$ ，意味着，X=0时，$odds\ ratio = 0$ ，X每增加一个单位 odds ration 增加 1.2（（X 增加2个单位odds ratio增加 2.4&mldr;.）</p><p><strong>求解</strong></p><p>通过上面的公式实际上不明白这些具体是什么，就可以通过求P来找到<strong>有结果的概率</strong>与<strong>截距</strong> $β_0$ 之间的关系，已知 $n=log_ab$ , $ a^n=b$ ，那么一个简单的逻辑回归公式为 $log(\frac{P}{1-P}) = \beta_0+\beta1X$ ，对这个公式进行推导：</p><ul><li>$\frac{P}{1-P} = e^{\beta_0+e^\beta1*X}$</li><li>$P = e^{\beta_0+e^\beta1<em>X} - Pe^{\beta_0+e^\beta1</em>X}$</li><li>$P(1+e^{\beta_0+e^\beta1<em>X}) = e^{\beta_0+e^\beta1</em>X}$</li><li>$P=\frac{e^{\beta_0+e^\beta1<em>X}}{1+e^{\beta_0+e^\beta1</em>X}}$</li></ul><p>当 $X=0$ ,则 $\beta_1*X$ 没意义，公式为：$P = frac{e^{β_0}}{(1+e^{β_0})}$ ，其中e是一个常数，python为 <code>math.e</code></p><p>如果单纯不算概率，只看截距符号，那么满足：</p><ul><li>如果截距为<strong>负号</strong>：则产生结果的概率将 &lt; 0.5。</li><li>如果截距为<strong>正号</strong>：那么产生结果的概率将 > 0.5。</li><li>如果截距<strong>等于 0</strong>：那么得到结果的概率正好是 0.5。</li></ul><p>通过例子来说明这点：假设研究为抽烟对心脏健康的影响，下表显示了一个逻辑回归</p><table><thead><tr><th></th><th>Coefficient</th><th>Standard Error</th><th>p-value</th></tr></thead><tbody><tr><td>Intercept</td><td>-1.93</td><td>0.13</td><td>&lt; 0.001</td></tr><tr><td>Smoking</td><td>0.38</td><td>0.17</td><td>0.03</td></tr></tbody></table><p>由表可知，截距为 -1.93，假设smoking系数为0，那么概率带入公式为：$P=\frac{e^{\beta_0}}{1+e^{\beta_0}} = P=\frac{e^{-1.93}}{1+e^{-1.93}} = 0.126$<code>(math.e ** -1.93)/(1+math.e ** -1.93)</code></p><p>如果 Smoking是一个连续变量（每年的吸烟量），在这种情况下，<code>Smoking=0</code> 意味着每年使用0公斤烟草的人即不抽烟的人群；那么这个结果就为，不抽烟的人群在未来10年内心脏有问题几率为 0.126。</p><p>再如果是吸烟者应该怎么计算，假设，每年吸烟量为3kg，那么公式为：$P = \frac{e^{β0 + β_1X}}{(1+e^{β0 + β_1X})}$ ，在这里 X=3，那么 $P=\frac{e^{\beta_1+\beta_2X}}{(1-e^{\beta_1+\beta_2X})} = \frac{e^{-1.93+0.38<em>3}}{(1-e^{-1.93+0.38</em>3})} = 0.31$ ；即得出，每年3KG烟草消耗量10年后有心脏问题的概率是 31%</p><blockquote><p><a href=https://quantifyinghealth.com/interpret-logistic-regression-intercept/ target=_blank rel="noopener nofollow noreferrer">interpret</a></p></blockquote><h3 id=sigmoid>sigmoid<a hidden class=anchor aria-hidden=true href=#sigmoid>#</a></h3><p>logit 函数的逆函数称Sigmoid 函数，sigmoid方程来源于 logit 为：$P=\frac{e^{log(odds)}}{(1-e^{log(odds)})} = \frac{1}{e^{-log(odds)+1}} = \frac{1}{1+e^{-z}}$ 。</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/220px-Kernel_Machine.svg.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/220px-Kernel_Machine.svg.png#center alt=具有线性支持向量机决策边界的散点图（虚线） onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><p>在python中，<code>np.exp</code> 是求 是求 $e^{x}$ 的值的函数。正好可以用在sigmod函数中，那么sigmoid可以写为</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>python</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>sigmoid</span><span class=p>(</span><span class=n>z</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>z</span><span class=p>))</span></span></span></code></pre></td></tr></table></div></div></div></div><h3 id=交叉熵或对数损失>交叉熵或对数损失<a hidden class=anchor aria-hidden=true href=#交叉熵或对数损失>#</a></h3><p>交叉熵 <strong>Cross-Entropy</strong>，通常用于量化两个概率分布之间的差异。用于逻辑回归，公式为：$H=\sum^{x=n}(P(x) \times log(q(x))$</p><h3 id=maximum-likelihood-estimation>Maximum Likelihood Estimation<a hidden class=anchor aria-hidden=true href=#maximum-likelihood-estimation>#</a></h3><p>最大似然估计，<code>Maximum Likelihood Estimation</code> <strong>MLE</strong>，是概率估算的一种解决方案。MLE在其中寻找一组参数，这些参数将影响数据样本 <em>X</em> 的联合概率的最佳拟合。</p><p>首先，定义一个称为 $\theta$ <em>theta</em> 的参数，该参数定义概率密度函数的选择和该分布的参数。它可能是一个数值向量，其值平滑变化并映射到不同的概率分布及其参数。在最大似然估计中，我们希望在给定特定概率分布及其参数的最大化情况下从联合概率分布中观察数据的概率，形式上表示为：$P(X|\theta)$ ，在这种情况下，条件概率通常使用分号 <strong>;</strong> 而不是竖线 <strong>|</strong> ，因为 $\theta$ 不是随机变量，而是未知参数。表达为 $P(X;\theta)$ ,或 $P(x_1,x_2,\ &mldr;\ x_n;\theta)$ 。</p><p>这样产生的条件概率被称为在给定模型参数 （$\theta$）的情况下观察变量 $X$ 的概率，并使用符号 <strong>L</strong> 来 表示似然函数。例如：$L(X;\theta)$。而<strong>最大似然估计</strong>的目标是找到使似然函数最大化的一组参数 ( $\theta$ )，例如产生最大似然值，如：$max(L(X;\theta))$</p><p>鉴于上述提到的变量 $X$ 是由n个样本组成，可以将其定义为在给定概率分布参数 $\theta$ 的情况下，变量 $X$ 的联合概率,如这里数据样本为 $x_1,x_2,\ &mldr;\ ,x_n$ 的联合概率，同时表示为 $L(x_1,x2,\ &mldr;\ ,x_n;\theta)$</p><p>大多数情况下，求解似然方程很复杂。会使用对数似然作为一种解决方案。由于对数函数是单调递增的，因此对数似然和似然中的最优参数是相同的。因此定义条件最大似然估计为：$log(P(x_i ; h))$。</p><p>用逻辑回归模型替换<em>h</em>，需要假设一个概率分布。在逻辑回归的情况下，假设数据样本为二项式概率分布，其中每个示例都是二项式的一个结果。伯努利分布只有一个参数：成功结果的概率 P，那么为：</p><ul><li>$P(y=1)=P$</li><li>$P(y=0)=1-P$</li></ul><p>那么这个平均值为：$P(y=1)*1+P(y=0)<em>0$，给出P的值公式可以转换为：$P</em>1+(1-p)*0$；这种公式看似没有意义，那么通过一个小例子来了解下</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>python</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 二项式似然函数</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>likelihood</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>p</span> <span class=o>*</span> <span class=n>y</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>p</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># test for y=1</span>
</span></span><span class=line><span class=cl><span class=n>y</span><span class=p>,</span> <span class=n>p</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span> <span class=mf>0.9</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;y=</span><span class=si>%.1f</span><span class=s1>, p=</span><span class=si>%.1f</span><span class=s1>, likelihood: </span><span class=si>%.3f</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>,</span> <span class=n>likelihood</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=n>y</span><span class=p>,</span> <span class=n>yhat</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;y=</span><span class=si>%.1f</span><span class=s1>, p=</span><span class=si>%.1f</span><span class=s1>, likelihood: </span><span class=si>%.3f</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>,</span> <span class=n>likelihood</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=c1># test for y=0</span>
</span></span><span class=line><span class=cl><span class=n>y</span><span class=p>,</span> <span class=n>yhat</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;y=</span><span class=si>%.1f</span><span class=s1>, p=</span><span class=si>%.1f</span><span class=s1>, likelihood: </span><span class=si>%.3f</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>,</span> <span class=n>likelihood</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=n>y</span><span class=p>,</span> <span class=n>yhat</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span> <span class=mf>0.9</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;y=</span><span class=si>%.1f</span><span class=s1>, p=</span><span class=si>%.1f</span><span class=s1>, likelihood: </span><span class=si>%.3f</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>,</span> <span class=n>likelihood</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># y=1.0, p=0.9, likelihood: 0.900</span>
</span></span><span class=line><span class=cl><span class=c1># y=1.0, p=0.9, likelihood: 0.900</span>
</span></span><span class=line><span class=cl><span class=c1># y=0.0, p=0.9, likelihood: 0.100</span>
</span></span><span class=line><span class=cl><span class=c1># y=0.0, p=0.9, likelihood: 0.100</span></span></span></code></pre></td></tr></table></div></div></div></div><p>运行示例会为每个案例打印类别y 和预测概率p，其中每个案例的概率是否接近；这里也可以使用对数更新似然函数，$log(p) * y + log(1 – p) * (1 – y)$；最后可以根据数据集中实例求最大似然和最小似然</p><ul><li>$\sum^{i=1}_n log(p_i) * y_i + log(1 – p_i) * (1 – y_i)$</li><li>最小似然使用反转函数，使负对数自然作为最小似然。上面的公式前加 <code>-</code></li></ul><p>对于计算二项式分布的对数似然相当于计算二项式分布[交叉熵，其中<code>P(class)</code>表示第 class 项概率，<code>q()</code> 表示概率分布，$-(log(q(class0)) \times P(class0) + log(q(class1)) * P(class1))$</p><h2 id=lr算法实例>LR算法实例<a hidden class=anchor aria-hidden=true href=#lr算法实例>#</a></h2><p>在研究如何从数据中估计模型的参数之前，我们需要了解逻辑回归准确计算的内容。</p><p>模型的线性部分（输入的加权和）计算成功事件的log-odds。</p><p>odds ratio：$\beta_0+\beta_1 \times x_1 + \beta_2 \times x_2\ &mldr;\ \beta_n \times x_n$ 该模型估计了每个级别的输入变量的log-odds。</p><p>由上面信息了解到，几率 <code>probability</code> 是输赢的比率 如 <code>1:10</code> ；<code>probability</code> 可以转换为 <code>odds ratio</code> 即成功概率除以不成功概率：$or=\frac{P}{1-P}$ ；计算or的对数，被称为log-odds是一种度量单位：$log(\frac{P}{1-P})$，而所求的即为 log-odds的逆函数，而在python中 <code>log</code> 函数是对数，求log的逆方法即 <code>exp</code> 返回n的x次方就是log的逆函数。</p><p>到这里已经和逻辑回归模型很接近了，对数函数公式可以简化为，$P=\frac{e^{log(odds)}}{(1-e^{log(odds)})}$ ，以上阐述了如何从log-odds转化为odds，然后在到逻辑回归模型。下面通过Python 中的示例来具体计算 <code>probability</code> 、<code>odds</code> 和 <code>log-odds</code> 之间的转换。假设将成功概率定义为 80% 或 0.8，然后将其转换为odds，然后再次转换为概率。</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>python</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>math</span> <span class=kn>import</span> <span class=n>log</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>math</span> <span class=kn>import</span> <span class=n>exp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prob</span> <span class=o>=</span> <span class=mf>0.8</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Probability </span><span class=si>%.1f</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 将 probability 转换为 odds</span>
</span></span><span class=line><span class=cl><span class=n>odds</span> <span class=o>=</span> <span class=n>prob</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Odds </span><span class=si>%.1f</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>odds</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 将 odds 转换为 log-odds</span>
</span></span><span class=line><span class=cl><span class=n>logodds</span> <span class=o>=</span> <span class=n>log</span><span class=p>(</span><span class=n>odds</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Log-Odds </span><span class=si>%.1f</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>logodds</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 转换 log-odds 为  probability</span>
</span></span><span class=line><span class=cl><span class=n>prob</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>logodds</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Probability </span><span class=si>%.1f</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Probability 0.8</span>
</span></span><span class=line><span class=cl><span class=c1># Odds 4.0</span>
</span></span><span class=line><span class=cl><span class=c1># Log-Odds 1.4</span>
</span></span><span class=line><span class=cl><span class=c1># Probability 0.8</span></span></span></code></pre></td></tr></table></div></div></div></div><p>通过这个例子，可以看到odds被转换成大约 1.4 的log-odds，然后正确地转回 0.8 的成功概率。</p><h2 id=逻辑回归实现>逻辑回归实现<a hidden class=anchor aria-hidden=true href=#逻辑回归实现>#</a></h2><p>首先将实现分为3个步骤：</p><ul><li>预测</li><li>评估系数</li><li>真实数据集预测</li></ul><h3 id=预测>预测<a hidden class=anchor aria-hidden=true href=#预测>#</a></h3><p>编写一个预测函数，在评估随机梯度下降中的候选系数值时以及在模型最终确定测试数据或新数据进行预测时。</p><p>下面是预测**predict()**函数，它预测给定一组系数的行的输出值。第一个系数是截距，也称为偏差或 b0，它是独立的，不负责输入值。</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>python</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>row</span><span class=p>,</span> <span class=n>coefficients</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>p</span> <span class=o>=</span> <span class=n>coefficients</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>row</span><span class=p>)</span><span class=o>-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>yhat</span> <span class=o>+=</span> <span class=n>coefficients</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=o>*</span> <span class=n>row</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>p</span><span class=p>))</span></span></span></code></pre></td></tr></table></div></div></div></div><p>准备一些测试数据，Y代表真实的类别</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>python</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>X1</span>					<span class=n>X2</span>						<span class=n>Y</span>
</span></span><span class=line><span class=cl><span class=mf>2.7810836</span> 	<span class=mf>2.550537003</span>		<span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=mf>1.465489372</span>	<span class=mf>2.362125076</span>		<span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=mf>3.396561688</span>	<span class=mf>4.400293529</span>		<span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=mf>1.38807019</span>	<span class=mf>1.850220317</span>		<span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=mf>3.06407232</span>	<span class=mf>3.005305973</span>		<span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=mf>7.627531214</span>	<span class=mf>2.759262235</span>		<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=mf>5.332441248</span>	<span class=mf>2.088626775</span>		<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=mf>6.922596716</span>	<span class=mf>1.77106367</span>		<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=mf>8.675418651</span>	<span class=o>-</span><span class=mf>0.242068655</span>	<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=mf>7.673756466</span>	<span class=mf>3.508563011</span>		<span class=mi>1</span></span></span></code></pre></td></tr></table></div></div></div></div><p>这里有两个输入值，和三个系数，系数是自定义的固定值，那么预测的公式就为</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>python</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 系数为</span>
</span></span><span class=line><span class=cl><span class=n>coef</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mf>0.406605464</span><span class=p>,</span> <span class=mf>0.852573316</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.104746259</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>e</span><span class=o>^</span><span class=p>(</span><span class=o>-</span><span class=p>(</span><span class=n>b0</span> <span class=o>+</span> <span class=n>b1</span> <span class=o>*</span> <span class=n>X1</span> <span class=o>+</span> <span class=n>b2</span> <span class=o>*</span> <span class=n>X2</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=c1># 套入公式（sigma）</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>e</span><span class=o>^</span><span class=p>(</span><span class=o>-</span><span class=p>(</span><span class=o>-</span><span class=mf>0.406605464</span> <span class=o>+</span> <span class=mf>0.852573316</span> <span class=o>*</span> <span class=n>X1</span> <span class=o>+</span> <span class=o>-</span><span class=mf>1.104746259</span> <span class=o>*</span> <span class=n>X2</span><span class=p>)))</span></span></span></code></pre></td></tr></table></div></div></div></div><p>完整的代码</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>python</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Make a prediction</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>math</span> <span class=kn>import</span> <span class=n>exp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Make a prediction with coefficients</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>row</span><span class=p>,</span> <span class=n>coefficients</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>yhat</span> <span class=o>=</span> <span class=n>coefficients</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>row</span><span class=p>)</span><span class=o>-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>yhat</span> <span class=o>+=</span> <span class=n>coefficients</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=o>*</span> <span class=n>row</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>yhat</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># test predictions</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=p>[[</span><span class=mf>2.7810836</span><span class=p>,</span><span class=mf>2.550537003</span><span class=p>,</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	<span class=p>[</span><span class=mf>1.465489372</span><span class=p>,</span><span class=mf>2.362125076</span><span class=p>,</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	<span class=p>[</span><span class=mf>3.396561688</span><span class=p>,</span><span class=mf>4.400293529</span><span class=p>,</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	<span class=p>[</span><span class=mf>1.38807019</span><span class=p>,</span><span class=mf>1.850220317</span><span class=p>,</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	<span class=p>[</span><span class=mf>3.06407232</span><span class=p>,</span><span class=mf>3.005305973</span><span class=p>,</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	<span class=p>[</span><span class=mf>7.627531214</span><span class=p>,</span><span class=mf>2.759262235</span><span class=p>,</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	<span class=p>[</span><span class=mf>5.332441248</span><span class=p>,</span><span class=mf>2.088626775</span><span class=p>,</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	<span class=p>[</span><span class=mf>6.922596716</span><span class=p>,</span><span class=mf>1.77106367</span><span class=p>,</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	<span class=p>[</span><span class=mf>8.675418651</span><span class=p>,</span><span class=o>-</span><span class=mf>0.242068655</span><span class=p>,</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>	<span class=p>[</span><span class=mf>7.673756466</span><span class=p>,</span><span class=mf>3.508563011</span><span class=p>,</span><span class=mi>1</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=n>coef</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mf>0.406605464</span><span class=p>,</span> <span class=mf>0.852573316</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.104746259</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>dataset</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	<span class=n>yhat</span> <span class=o>=</span> <span class=n>predict</span><span class=p>(</span><span class=n>row</span><span class=p>,</span> <span class=n>coef</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Expected=</span><span class=si>%.3f</span><span class=s2>, Predicted=</span><span class=si>%.3f</span><span class=s2> [</span><span class=si>%d</span><span class=s2>]&#34;</span> <span class=o>%</span> <span class=p>(</span><span class=n>row</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>yhat</span><span class=p>,</span> <span class=nb>round</span><span class=p>(</span><span class=n>yhat</span><span class=p>)))</span></span></span></code></pre></td></tr></table></div></div></div></div><h3 id=估计系数>估计系数<a hidden class=anchor aria-hidden=true href=#估计系数>#</a></h3><p>这里可以使用我随机梯度下降来估计训练数据的系数值。随机梯度下降需要两个参数：</p><ul><li><strong>学习率</strong> Learning rate：用于限制每个系数每次更新时的修正量。</li><li><strong>Epochs</strong>：更新系数时遍历训练数据的次数。</li></ul><p>在每个epoch更新训练数据中每一行的每个系数。系数会根据模型产生的错误进行更新，误差为预期输出与预测值之间的差异。错误会随着epoch增加而减少</p><p>将每个都加权，并且这些系数以一致的方式进行更新，用公式可以表示为</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>python</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>b1</span><span class=p>(</span><span class=n>t</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span> <span class=o>=</span> <span class=n>b1</span><span class=p>(</span><span class=n>t</span><span class=p>)</span> <span class=o>+</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=p>(</span><span class=n>y</span><span class=p>(</span><span class=n>t</span><span class=p>)</span> <span class=o>-</span> <span class=n>p</span><span class=p>(</span><span class=n>t</span><span class=p>))</span> <span class=o>*</span> <span class=n>p</span><span class=p>(</span><span class=n>t</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>p</span><span class=p>(</span><span class=n>t</span><span class=p>))</span> <span class=o>*</span> <span class=n>x1</span><span class=p>(</span><span class=n>t</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div></div></div><p>那么整合一起为</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>python</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>math</span> <span class=kn>import</span> <span class=n>exp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 预测函数</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>row</span><span class=p>,</span> <span class=n>coefficients</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>coefficients</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>row</span><span class=p>)</span><span class=o>-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>p</span> <span class=o>+=</span> <span class=n>coefficients</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=o>*</span> <span class=n>row</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>p</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>coefficients_sgd</span><span class=p>(</span><span class=n>train</span><span class=p>,</span> <span class=n>l_rate</span><span class=p>,</span> <span class=n>n_epoch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>coef</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.0</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train</span><span class=p>[</span><span class=mi>0</span><span class=p>]))]</span> <span class=c1># 初始一个系数，第一次为都为0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_epoch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>sum_error</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>train</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>p</span> <span class=o>=</span> <span class=n>predict</span><span class=p>(</span><span class=n>row</span><span class=p>,</span> <span class=n>coef</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 错误为预期值与实际值直接差异</span>
</span></span><span class=line><span class=cl>            <span class=n>error</span> <span class=o>=</span> <span class=n>row</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>p</span>
</span></span><span class=line><span class=cl>            <span class=n>sum_error</span> <span class=o>+=</span> <span class=n>error</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>            <span class=c1># 截距没有输入变量x，这里为row[0]</span>
</span></span><span class=line><span class=cl>            <span class=n>coef</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>coef</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>l_rate</span> <span class=o>*</span> <span class=n>error</span> <span class=o>*</span> <span class=n>p</span> <span class=o>*</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>row</span><span class=p>)</span><span class=o>-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=c1># 其他系数更新</span>
</span></span><span class=line><span class=cl>                <span class=n>coef</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>coef</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>l_rate</span> <span class=o>*</span> <span class=n>error</span> <span class=o>*</span> <span class=n>p</span> <span class=o>*</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>p</span><span class=p>)</span> <span class=o>*</span> <span class=n>row</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;&gt;epoch=</span><span class=si>%d</span><span class=s1>, lrate=</span><span class=si>%.3f</span><span class=s1>, error=</span><span class=si>%.3f</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=p>(</span><span class=n>epoch</span><span class=p>,</span> <span class=n>l_rate</span><span class=p>,</span> <span class=n>sum_error</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>coef</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Calculate coefficients</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>2.7810836</span><span class=p>,</span><span class=mf>2.550537003</span><span class=p>,</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>1.465489372</span><span class=p>,</span><span class=mf>2.362125076</span><span class=p>,</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>3.396561688</span><span class=p>,</span><span class=mf>4.400293529</span><span class=p>,</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>1.38807019</span><span class=p>,</span><span class=mf>1.850220317</span><span class=p>,</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>3.06407232</span><span class=p>,</span><span class=mf>3.005305973</span><span class=p>,</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>7.627531214</span><span class=p>,</span><span class=mf>2.759262235</span><span class=p>,</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>5.332441248</span><span class=p>,</span><span class=mf>2.088626775</span><span class=p>,</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>6.922596716</span><span class=p>,</span><span class=mf>1.77106367</span><span class=p>,</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>8.675418651</span><span class=p>,</span><span class=o>-</span><span class=mf>0.242068655</span><span class=p>,</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>7.673756466</span><span class=p>,</span><span class=mf>3.508563011</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>l_rate</span> <span class=o>=</span> <span class=mf>0.3</span>
</span></span><span class=line><span class=cl><span class=n>n_epoch</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=n>coef</span> <span class=o>=</span> <span class=n>coefficients_sgd</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>l_rate</span><span class=p>,</span> <span class=n>n_epoch</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>coef</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># &gt;epoch=92, lrate=0.300, error=0.024</span>
</span></span><span class=line><span class=cl><span class=c1># &gt;epoch=93, lrate=0.300, error=0.024</span>
</span></span><span class=line><span class=cl><span class=c1># &gt;epoch=94, lrate=0.300, error=0.024</span>
</span></span><span class=line><span class=cl><span class=c1># &gt;epoch=95, lrate=0.300, error=0.023</span>
</span></span><span class=line><span class=cl><span class=c1># &gt;epoch=96, lrate=0.300, error=0.023</span>
</span></span><span class=line><span class=cl><span class=c1># &gt;epoch=97, lrate=0.300, error=0.023</span>
</span></span><span class=line><span class=cl><span class=c1># &gt;epoch=98, lrate=0.300, error=0.023</span>
</span></span><span class=line><span class=cl><span class=c1># &gt;epoch=99, lrate=0.300, error=0.022</span>
</span></span><span class=line><span class=cl><span class=c1>#[-0.8596443546618897, 1.5223825112460005, -2.218700210565016]</span></span></span></code></pre></td></tr></table></div></div></div></div><p>这里跟踪了跟踪每个epoch误差平方的总和，以便我们可以在每个epoch中打印出error，实例中使用 0.3 学习率并训练100 个 epoch，每个epoch会打印出其误差平方，最终会打印总系数集</p><h3 id=套用真实数据集>套用真实数据集<a hidden class=anchor aria-hidden=true href=#套用真实数据集>#</a></h3><p><a href=https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv target=_blank rel="noopener nofollow noreferrer">糖尿病数据集</a> 是根据基本的医疗信息，预测印第安人5年内患糖尿病的情况。这是一个二元分类，阴性0与阳性1直接的关系。采用了二项式分布，也可以采用其他分布，如高斯等。</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>python</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span><span class=lnt>116
</span><span class=lnt>117
</span><span class=lnt>118
</span><span class=lnt>119
</span><span class=lnt>120
</span><span class=lnt>121
</span><span class=lnt>122
</span><span class=lnt>123
</span><span class=lnt>124
</span><span class=lnt>125
</span><span class=lnt>126
</span><span class=lnt>127
</span><span class=lnt>128
</span><span class=lnt>129
</span><span class=lnt>130
</span><span class=lnt>131
</span><span class=lnt>132
</span><span class=lnt>133
</span><span class=lnt>134
</span><span class=lnt>135
</span><span class=lnt>136
</span><span class=lnt>137
</span><span class=lnt>138
</span><span class=lnt>139
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>random</span> <span class=kn>import</span> <span class=n>seed</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>random</span> <span class=kn>import</span> <span class=n>randrange</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>csv</span> <span class=kn>import</span> <span class=n>reader</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>math</span> <span class=kn>import</span> <span class=n>exp</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load a CSV file</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>load_csv</span><span class=p>(</span><span class=n>filename</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>dataset</span> <span class=o>=</span> <span class=nb>list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>file</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=n>csv_reader</span> <span class=o>=</span> <span class=n>reader</span><span class=p>(</span><span class=n>file</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>csv_reader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=k>if</span> <span class=ow>not</span> <span class=n>row</span><span class=p>:</span>
</span></span><span class=line><span class=cl>				<span class=k>continue</span>
</span></span><span class=line><span class=cl>			<span class=n>dataset</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>row</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>dataset</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Convert string column to float</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>str_column_to_float</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>column</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>dataset</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=n>row</span><span class=p>[</span><span class=n>column</span><span class=p>]</span> <span class=o>=</span> <span class=nb>float</span><span class=p>(</span><span class=n>row</span><span class=p>[</span><span class=n>column</span><span class=p>]</span><span class=o>.</span><span class=n>strip</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 找到最小和最大的</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>dataset_minmax</span><span class=p>(</span><span class=n>dataset</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>minmax</span> <span class=o>=</span> <span class=nb>list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>])):</span>
</span></span><span class=line><span class=cl>		<span class=n>col_values</span> <span class=o>=</span> <span class=p>[</span><span class=n>row</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>dataset</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=n>value_min</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=n>col_values</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>value_max</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>col_values</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>minmax</span><span class=o>.</span><span class=n>append</span><span class=p>([</span><span class=n>value_min</span><span class=p>,</span> <span class=n>value_max</span><span class=p>])</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>minmax</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 归一化</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>normalize_dataset</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>minmax</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>dataset</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>row</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>			<span class=n>row</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=n>row</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>minmax</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>])</span> <span class=o>/</span> <span class=p>(</span><span class=n>minmax</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>minmax</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=c1># k-folds CV实现</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>cross_validation_split</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>n_folds</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>dataset_split</span> <span class=o>=</span> <span class=nb>list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=n>dataset_copy</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>dataset</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>fold_size</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=p>)</span> <span class=o>/</span> <span class=n>n_folds</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_folds</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>fold</span> <span class=o>=</span> <span class=nb>list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=k>while</span> <span class=nb>len</span><span class=p>(</span><span class=n>fold</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>fold_size</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=n>index</span> <span class=o>=</span> <span class=n>randrange</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>dataset_copy</span><span class=p>))</span>
</span></span><span class=line><span class=cl>			<span class=n>fold</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>dataset_copy</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=n>index</span><span class=p>))</span>
</span></span><span class=line><span class=cl>		<span class=n>dataset_split</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>fold</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>dataset_split</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 计算准确度百分比</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>accuracy_metric</span><span class=p>(</span><span class=n>actual</span><span class=p>,</span> <span class=n>predicted</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>correct</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>actual</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=n>actual</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>==</span> <span class=n>predicted</span><span class=p>[</span><span class=n>i</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>			<span class=n>correct</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>correct</span> <span class=o>/</span> <span class=nb>float</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>actual</span><span class=p>))</span> <span class=o>*</span> <span class=mf>100.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用CV评估算法</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>evaluate_algorithm</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>algorithm</span><span class=p>,</span> <span class=n>n_folds</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>folds</span> <span class=o>=</span> <span class=n>cross_validation_split</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>n_folds</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>scores</span> <span class=o>=</span> <span class=nb>list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>fold</span> <span class=ow>in</span> <span class=n>folds</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=n>train_set</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>folds</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>train_set</span><span class=o>.</span><span class=n>remove</span><span class=p>(</span><span class=n>fold</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>train_set</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span><span class=n>train_set</span><span class=p>,</span> <span class=p>[])</span>
</span></span><span class=line><span class=cl>		<span class=n>test_set</span> <span class=o>=</span> <span class=nb>list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>fold</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=n>row_copy</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>row</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			<span class=n>test_set</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>row_copy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			<span class=n>row_copy</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>		<span class=n>predicted</span> <span class=o>=</span> <span class=n>algorithm</span><span class=p>(</span><span class=n>train_set</span><span class=p>,</span> <span class=n>test_set</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>actual</span> <span class=o>=</span> <span class=p>[</span><span class=n>row</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>fold</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=n>accuracy</span> <span class=o>=</span> <span class=n>accuracy_metric</span><span class=p>(</span><span class=n>actual</span><span class=p>,</span> <span class=n>predicted</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>scores</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>accuracy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>scores</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用系数进行预测</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>row</span><span class=p>,</span> <span class=n>coefficients</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>yhat</span> <span class=o>=</span> <span class=n>coefficients</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>row</span><span class=p>)</span><span class=o>-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>yhat</span> <span class=o>+=</span> <span class=n>coefficients</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=o>*</span> <span class=n>row</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>yhat</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 系数生成</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>coefficients_sgd</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>train</span><span class=p>,</span> <span class=n>l_rate</span><span class=p>,</span> <span class=n>n_epoch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    生成系数
</span></span></span><span class=line><span class=cl><span class=s2>    :param train: list, 数据集，可以是训练集
</span></span></span><span class=line><span class=cl><span class=s2>    :param l_rate: float, 学习率
</span></span></span><span class=line><span class=cl><span class=s2>    :param n_epoch:int，epoch，这里代表进行多少次迭代
</span></span></span><span class=line><span class=cl><span class=s2>    :return: None
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>coef</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.0</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>train</span><span class=p>[</span><span class=mi>0</span><span class=p>]))]</span> <span class=c1># 初始一个系数，第一次为都为0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_epoch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>sum_error</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>train</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>p</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>row</span><span class=p>,</span> <span class=n>coef</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 错误为预期值与实际值直接差异</span>
</span></span><span class=line><span class=cl>            <span class=n>error</span> <span class=o>=</span> <span class=n>row</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=n>p</span>
</span></span><span class=line><span class=cl>            <span class=n>sum_error</span> <span class=o>+=</span> <span class=n>error</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>            <span class=c1># 截距没有输入变量x，这里为row[0]</span>
</span></span><span class=line><span class=cl>            <span class=n>coef</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>coef</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>l_rate</span> <span class=o>*</span> <span class=n>error</span> <span class=o>*</span> <span class=n>p</span> <span class=o>*</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>row</span><span class=p>)</span><span class=o>-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=c1># 其他系数更新</span>
</span></span><span class=line><span class=cl>                <span class=n>coef</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>coef</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>l_rate</span> <span class=o>*</span> <span class=n>error</span> <span class=o>*</span> <span class=n>p</span> <span class=o>*</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=n>p</span><span class=p>)</span> <span class=o>*</span> <span class=n>row</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=c1># print(&#39;&gt;epoch=%d, lrate=%.3f, error=%.3f&#39; % (epoch, l_rate, sum_error))</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>coef</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 随机梯度下降的逻辑回归算法</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>logistic_regression</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>train</span><span class=p>,</span> <span class=n>test</span><span class=p>,</span> <span class=n>l_rate</span><span class=p>,</span> <span class=n>n_epoch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>predictions</span> <span class=o>=</span> <span class=nb>list</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>coef</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>coefficients_sgd</span><span class=p>(</span><span class=n>train</span><span class=p>,</span> <span class=n>l_rate</span><span class=p>,</span> <span class=n>n_epoch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>test</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>p</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>row</span><span class=p>,</span> <span class=n>coef</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>p</span> <span class=o>=</span> <span class=nb>round</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>predictions</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span><span class=p>(</span><span class=n>predictions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>seed</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 数据预处理</span>
</span></span><span class=line><span class=cl><span class=n>filename</span> <span class=o>=</span> <span class=s1>&#39;pima-indians-diabetes.csv&#39;</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>load_csv</span><span class=p>(</span><span class=n>filename</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>dataset</span><span class=p>[</span><span class=mi>0</span><span class=p>])):</span>
</span></span><span class=line><span class=cl>	<span class=n>str_column_to_float</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>i</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 做归一化</span>
</span></span><span class=line><span class=cl><span class=n>minmax</span> <span class=o>=</span> <span class=n>dataset_minmax</span><span class=p>(</span><span class=n>dataset</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>normalize_dataset</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>minmax</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># evaluate algorithm</span>
</span></span><span class=line><span class=cl><span class=n>n_folds</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl><span class=n>l_rate</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl><span class=n>n_epoch</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=n>scores</span> <span class=o>=</span> <span class=n>evaluate_algorithm</span><span class=p>(</span><span class=n>dataset</span><span class=p>,</span> <span class=n>logistic_regression</span><span class=p>,</span> <span class=n>n_folds</span><span class=p>,</span> <span class=n>l_rate</span><span class=p>,</span> <span class=n>n_epoch</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Scores: </span><span class=si>%s</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>scores</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Mean Accuracy: </span><span class=si>%.3f%%</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=p>(</span><span class=nb>sum</span><span class=p>(</span><span class=n>scores</span><span class=p>)</span><span class=o>/</span><span class=nb>float</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>scores</span><span class=p>))))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 0.35294117647058826</span>
</span></span><span class=line><span class=cl><span class=c1># Scores: [73.8562091503268, 78.43137254901961, 81.69934640522875, 75.81699346405229, 75.81699346405229]</span>
</span></span><span class=line><span class=cl><span class=c1># Mean Accuracy: 77.124%</span></span></span></code></pre></td></tr></table></div></div></div></div><p>上述是对整个数据集的预测百分比，也可以对对应的类的信息进行输出</p><h2 id=reference-1>Reference<a hidden class=anchor aria-hidden=true href=#reference-1>#</a></h2><blockquote><p><a href=https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1 target=_blank rel="noopener nofollow noreferrer">Maximum likelihood estimation</a></p><p><a href=https://vitalflux.com/logistic-regression-sigmoid-function-python-code/ target=_blank rel="noopener nofollow noreferrer">Sigmoid Function</a></p><p><a href=https://christophm.github.io/interpretable-ml-book/logistic.html target=_blank rel="noopener nofollow noreferrer">logistic</a></p><p><a href=https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#binary-logistic-regression target=_blank rel="noopener nofollow noreferrer">binary logistic regression</a></p><p><a href="https://machinelearningmastery.com/?s=Logistic+Regression&amp;post_type=post&amp;submit=Search" target=_blank rel="noopener nofollow noreferrer">LR implementation</a></p></blockquote></div><div class=pe-copyright><hr><blockquote><p>本文为原创内容，版权归作者所有。如需转载，请在文章中声明本文标题及链接。</p><p>文章标题：逻辑回归</p><p>文章链接：<a href=https://www.oomkill.com/2022/06/logistic-regression/ target=_blank>https://www.oomkill.com/2022/06/logistic-regression/</a></p><p>许可协议：<a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></p></blockquote></div><div class=comments-separator></div><h3 class=relatedContentTitle>相关阅读</h3><ul class=relatedContent><li><a href=/2022/06/knn/><span>KNN算法</span></a></li><li><a href=/2022/06/decision-tree/><span>决策树</span></a></li><li><a href=/2022/06/decision-boundary/><span>决策边界算法</span></a></li><li><a href=/2022/06/naive-bayes/><span>朴素贝叶斯算法</span></a></li><li><a href=/2016/09/consistent-hash/><span>一致性hash在memcache中的应用</span></a></li></ul><div class=comments-separator></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.oomkill.com/tags/machinelearning/>MachineLearning</a></li><li><a href=https://www.oomkill.com/tags/algorithm/>Algorithm</a></li><li><a href=https://www.oomkill.com/tags/cs/>CS</a></li></ul><nav class=paginav><a class=prev href=https://www.oomkill.com/2022/06/decision-tree/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></polyline></svg>&nbsp;</span>
<span>决策树</span>
</a><a class=next href=https://www.oomkill.com/2022/06/naive-bayes/><span class=title></span>
<span>朴素贝叶斯算法&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span></a></nav></footer><div class=pe-comments-decoration><p class=pe-comments-title></p><p class=pe-comments-subtitle></p></div><div id=pe-comments></div><script src=/js/pe-go-comment.min.86a214102576ba5f9b7bdc29eed8d58dd56e34aef80b3c65c73ea9cc88443696.js integrity="sha256-hqIUECV2ul+be9wp7tjVjdVuNK74Czxlxz6pzIhENpY="></script><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="dark"?"dark":"light",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"cylonchau/cylonchau.github.io","data-repo-id":"R_kgDOIRlNSQ","data-category":"Announcements","data-category-id":"DIC_kwDOIRlNSc4CXy1U","data-mapping":"pathname","data-term":"posts/logistic-regression","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":getStoredTheme(),"data-lang":"zh-TW","data-loading":"lazy",crossorigin:"anonymous",async:""},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#pe-comments").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2024 <a href=https://www.oomkill.com/>Cylon's Collection</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> on
<a href=https://pages.github.com/ rel=noopener target=_blank>GitHub Pages</a> & Theme
        <a href=https://github.com/tofuwine/PaperMod-PE rel=noopener target=_blank>PaperMod-PE</a></span></footer><div class=pe-right-sidebar><a href=javascript:void(0); id=theme-toggle-float class=pe-float-btn><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</a><a href=#top class=pe-float-btn id=top-link><span id=pe-read-progress></span></a></div><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>