<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>决策边界算法 | Cylon's Collection</title>
<meta name=keywords content="MachineLearning,algorithm,CS"><meta name=description content="决策边界 (decision boundary)
支持向量机获取这些数据点并输出最能分离标签的超平面。这条线是决策边界
决策平面 （ decision surface ），是将空间划分为不同的区域。位于决策平面一侧的数据被定义为与位于另一侧的数据属于不同的类别。决策面可以作为学习过程的结果创建或修改，它们经常用于机器学习、模式识别和分类系统。
环境空间 ( Ambient Space)，围绕数学对象即对象本身的空间，如一维 Line ，可以独立研究，这种情况下L则是L；再例如将L作为二维空间 $R^2$ 的对象进行研究，这种情况下 L 的环境空间是 $R^2$。
超平面（Hyperplane）是一个子空间， N维空间的超平面是其具有维数的平面的子集。就其性质而言，它将空间分成两个半空间，其维度比其环境空间的维度小 1。如果空间是三维的，那么它的超平面就是二维维平面，而如果空间是 2 维的，那么它的超平面就是一维线。支持向量机 (SVM) 通过找到使两个类之间的边距最大化的超平面来执行分类。
法向量 （Normal） 是垂直于该平面、另一个向量的 90° 角倾斜
什么是支持向量 支持向量 （Support vectors），靠近决策平面（超平面）的数据点。
如图所示，从一维平面来看，哪个是分离的超平面？
一般而言，会有很多种解决方法（超平面），支持向量机就是如何找到最佳方法的解决方案。
转置运算
矩阵的转置是原始矩阵的翻转版本，可以通过转换矩阵的行和列来转置矩阵。我们用 $A^T$ 表示矩阵 A 的转置。例如，
$$A=\left[ \begin{matrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ \end{matrix} \right]$$ ；那么 A 的转置就为 $$A=\left[ \begin{matrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \\ \end{matrix} \right]$$ ；"><meta name=author content="cylon"><link rel=canonical href=https://www.oomkill.com/2022/06/decision-boundary/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://www.oomkill.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.oomkill.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.oomkill.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.oomkill.com/favicon.ico><link rel=mask-icon href=https://www.oomkill.com/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://www.oomkill.com/2022/06/decision-boundary/><noscript><style>#theme-toggle,#top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=/assets/css/pe.min.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/pe.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/v4-shims.min.css><script defer src=https://cdn.staticfile.net/jquery/3.5.1/jquery.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/fancybox/3.5.7/jquery.fancybox.min.css><script defer src=https://cdn.staticfile.net/fancybox/3.5.7/jquery.fancybox.min.js></script><script id=MathJax-script async src=https://cdn.staticfile.net/mathjax/3.2.2/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["\\$","\\$"]]}}</script><meta property="og:title" content="决策边界算法"><meta property="og:description" content="决策边界 (decision boundary)
支持向量机获取这些数据点并输出最能分离标签的超平面。这条线是决策边界
决策平面 （ decision surface ），是将空间划分为不同的区域。位于决策平面一侧的数据被定义为与位于另一侧的数据属于不同的类别。决策面可以作为学习过程的结果创建或修改，它们经常用于机器学习、模式识别和分类系统。
环境空间 ( Ambient Space)，围绕数学对象即对象本身的空间，如一维 Line ，可以独立研究，这种情况下L则是L；再例如将L作为二维空间 $R^2$ 的对象进行研究，这种情况下 L 的环境空间是 $R^2$。
超平面（Hyperplane）是一个子空间， N维空间的超平面是其具有维数的平面的子集。就其性质而言，它将空间分成两个半空间，其维度比其环境空间的维度小 1。如果空间是三维的，那么它的超平面就是二维维平面，而如果空间是 2 维的，那么它的超平面就是一维线。支持向量机 (SVM) 通过找到使两个类之间的边距最大化的超平面来执行分类。
法向量 （Normal） 是垂直于该平面、另一个向量的 90° 角倾斜
什么是支持向量 支持向量 （Support vectors），靠近决策平面（超平面）的数据点。
如图所示，从一维平面来看，哪个是分离的超平面？
一般而言，会有很多种解决方法（超平面），支持向量机就是如何找到最佳方法的解决方案。
转置运算
矩阵的转置是原始矩阵的翻转版本，可以通过转换矩阵的行和列来转置矩阵。我们用 $A^T$ 表示矩阵 A 的转置。例如，
$$A=\left[ \begin{matrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ \end{matrix} \right]$$ ；那么 A 的转置就为 $$A=\left[ \begin{matrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \\ \end{matrix} \right]$$ ；"><meta property="og:type" content="article"><meta property="og:url" content="https://www.oomkill.com/2022/06/decision-boundary/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-22T23:00:36+08:00"><meta property="og:site_name" content="Cylon's Collection"><meta name=twitter:card content="summary"><meta name=twitter:title content="决策边界算法"><meta name=twitter:description content="决策边界 (decision boundary)
支持向量机获取这些数据点并输出最能分离标签的超平面。这条线是决策边界
决策平面 （ decision surface ），是将空间划分为不同的区域。位于决策平面一侧的数据被定义为与位于另一侧的数据属于不同的类别。决策面可以作为学习过程的结果创建或修改，它们经常用于机器学习、模式识别和分类系统。
环境空间 ( Ambient Space)，围绕数学对象即对象本身的空间，如一维 Line ，可以独立研究，这种情况下L则是L；再例如将L作为二维空间 $R^2$ 的对象进行研究，这种情况下 L 的环境空间是 $R^2$。
超平面（Hyperplane）是一个子空间， N维空间的超平面是其具有维数的平面的子集。就其性质而言，它将空间分成两个半空间，其维度比其环境空间的维度小 1。如果空间是三维的，那么它的超平面就是二维维平面，而如果空间是 2 维的，那么它的超平面就是一维线。支持向量机 (SVM) 通过找到使两个类之间的边距最大化的超平面来执行分类。
法向量 （Normal） 是垂直于该平面、另一个向量的 90° 角倾斜
什么是支持向量 支持向量 （Support vectors），靠近决策平面（超平面）的数据点。
如图所示，从一维平面来看，哪个是分离的超平面？
一般而言，会有很多种解决方法（超平面），支持向量机就是如何找到最佳方法的解决方案。
转置运算
矩阵的转置是原始矩阵的翻转版本，可以通过转换矩阵的行和列来转置矩阵。我们用 $A^T$ 表示矩阵 A 的转置。例如，
$$A=\left[ \begin{matrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ \end{matrix} \right]$$ ；那么 A 的转置就为 $$A=\left[ \begin{matrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \\ \end{matrix} \right]$$ ；"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.oomkill.com/posts/"},{"@type":"ListItem","position":2,"name":"决策边界算法","item":"https://www.oomkill.com/2022/06/decision-boundary/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"决策边界算法","name":"决策边界算法","description":"决策边界 (decision boundary)\n支持向量机获取这些数据点并输出最能分离标签的超平面。这条线是决策边界\n决策平面 （ decision surface ），是将空间划分为不同的区域。位于决策平面一侧的数据被定义为与位于另一侧的数据属于不同的类别。决策面可以作为学习过程的结果创建或修改，它们经常用于机器学习、模式识别和分类系统。\n环境空间 ( Ambient Space)，围绕数学对象即对象本身的空间，如一维 Line ，可以独立研究，这种情况下L则是L；再例如将L作为二维空间 $R^2$ 的对象进行研究，这种情况下 L 的环境空间是 $R^2$。\n超平面（Hyperplane）是一个子空间， N维空间的超平面是其具有维数的平面的子集。就其性质而言，它将空间分成两个半空间，其维度比其环境空间的维度小 1。如果空间是三维的，那么它的超平面就是二维维平面，而如果空间是 2 维的，那么它的超平面就是一维线。支持向量机 (SVM) 通过找到使两个类之间的边距最大化的超平面来执行分类。\n法向量 （Normal） 是垂直于该平面、另一个向量的 90° 角倾斜\n什么是支持向量 支持向量 （Support vectors），靠近决策平面（超平面）的数据点。\n如图所示，从一维平面来看，哪个是分离的超平面？\n一般而言，会有很多种解决方法（超平面），支持向量机就是如何找到最佳方法的解决方案。\n转置运算\n矩阵的转置是原始矩阵的翻转版本，可以通过转换矩阵的行和列来转置矩阵。我们用 $A^T$ 表示矩阵 A 的转置。例如，\n$$A=\\left[ \\begin{matrix} 1 \u0026 2 \u0026 3 \\\\ 4 \u0026 5 \u0026 6 \\\\ \\end{matrix} \\right]$$ ；那么 A 的转置就为 $$A=\\left[ \\begin{matrix} 1 \u0026 4 \\\\ 2 \u0026 5 \\\\ 3 \u0026 6 \\\\ \\end{matrix} \\right]$$ ；","keywords":["MachineLearning","algorithm","CS"],"articleBody":"决策边界 (decision boundary)\n支持向量机获取这些数据点并输出最能分离标签的超平面。这条线是决策边界\n决策平面 （ decision surface ），是将空间划分为不同的区域。位于决策平面一侧的数据被定义为与位于另一侧的数据属于不同的类别。决策面可以作为学习过程的结果创建或修改，它们经常用于机器学习、模式识别和分类系统。\n环境空间 ( Ambient Space)，围绕数学对象即对象本身的空间，如一维 Line ，可以独立研究，这种情况下L则是L；再例如将L作为二维空间 $R^2$ 的对象进行研究，这种情况下 L 的环境空间是 $R^2$。\n超平面（Hyperplane）是一个子空间， N维空间的超平面是其具有维数的平面的子集。就其性质而言，它将空间分成两个半空间，其维度比其环境空间的维度小 1。如果空间是三维的，那么它的超平面就是二维维平面，而如果空间是 2 维的，那么它的超平面就是一维线。支持向量机 (SVM) 通过找到使两个类之间的边距最大化的超平面来执行分类。\n法向量 （Normal） 是垂直于该平面、另一个向量的 90° 角倾斜\n什么是支持向量 支持向量 （Support vectors），靠近决策平面（超平面）的数据点。\n如图所示，从一维平面来看，哪个是分离的超平面？\n一般而言，会有很多种解决方法（超平面），支持向量机就是如何找到最佳方法的解决方案。\n转置运算\n矩阵的转置是原始矩阵的翻转版本，可以通过转换矩阵的行和列来转置矩阵。我们用 $A^T$ 表示矩阵 A 的转置。例如，\n$$A=\\left[ \\begin{matrix} 1 \u0026 2 \u0026 3 \\\\ 4 \u0026 5 \u0026 6 \\\\ \\end{matrix} \\right]$$ ；那么 A 的转置就为 $$A=\\left[ \\begin{matrix} 1 \u0026 4 \\\\ 2 \u0026 5 \\\\ 3 \u0026 6 \\\\ \\end{matrix} \\right]$$ ；\n我们可以将向量的转置作为特例。由于 n 维向量 x 由 n×1 列矩阵表示：\n$$x=\\left[ \\begin{matrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ .... \\\\ x_n \\\\ \\end{matrix} \\right]$$ ；那么 x 的转置（$x^T$）是一个 $1\\times n$ 行矩阵 $$x^T=\\left[ \\begin{matrix} x_1 \u0026 x_2 \u0026 x_3 \u0026 ... \u0026 x_n \\\\ \\end{matrix} \\right]$$ 。\n权重向量\n$wx+b=0$ w：权重向量 x n维向量 $x_i=[1,2,3…n]$ $w_i=[1,2,3…n]$ 每个输入的值都乘以一个“权重” $w_i$。权重是表示计算输出时每个输入的重要性的值\n权重决定了输入对输出的影响程度。 $Y=\\sum(Weight \\times input)+bias$ ；如果输入为 $[x_1,x_2\\ … ,x_n]$ 权重是：$[w_1,w_2\\ \\ ,w_n]$\n通过场景来理解\n假设预估汽车的价格，汽车的价格取决于制造年份和行驶里程数。让我们假设汽车的年份越高，汽车价格越高。随后，汽车开得越多，汽车就越便宜。\n这个例子应该可以帮助您了解汽车价格与制造年份之间存在正相关关系，而汽车价格与其行驶里程之间存在负关关系。因此，我们希望看到代表年份的特征的权重为正，代表里程的特征的权重为负。公式为：$car = (w_1x\\ ear+w_2x\\ miles)$\n偏差 bais 是一个常数 const ，偏差用于将影响函数的结果向正或负方向移动。bias 会被被添加到 input 和 weight 的乘积中。偏差用于抵消结果。$x_1w_1+x_2w_2…x_nw_n+bias$\n通过场景来理解\n假设希望在输入为 0 时返回 2。由于权重和输入的乘积之和为 0，您将如何确保返回 2？此时可以添加2的bias。如果不包含偏差，只是对 input 和 weight 执行矩阵乘法。这将很容易导致过度拟合数据集。\n过度拟合（overfitting）是指机器学习算模型在训练集上的误差和测试集上的误差之间差异过大。造成过度拟合的原因可能有多种．最常见的就是模型容量过高，模型过于复杂，换句话说是模型假设所包含的参数数量过多．如此一来，算法会将训练集中所包含的没有普遍性的一些特征也学习进来，结果降低了模型的泛化能力．\nhttps://machine-learning.paperspace.com/wiki/weights-and-biases\n范数\n向量的范数（norm）是它的长度 ，x的范数表示为 $\\parallel x \\parallel$；常用的范数为 P 范数，其中 P 是大于等于1的任何数，向量 x的 p 范数表示为 $\\parallel x \\parallel_p$ ；通常情况下向量 x 的 p 范数的计算公式为： $\\parallel x \\parallel_p = (x_1^p+x_2^p+x_3^p+ \\ …\\ x_n^p)^{1/p}$ ；公式可以简写为：$\\parallel x \\parallel_p = (\\sum_{i=1}^n\\ x_i^p)^{1/p}$\n曼哈顿距离\n曼哈顿距离也被称为1-范数 1-norm，因为它测量的中两点之间的距离。假设：向量a，我们必须计算 1-范式 $\\vec{a} = [2,3]$ ，在图像中表示（红色线部分表示向量a的1-范式）\n通过公式来计算1-范式，可以将p替换为1，$\\parallel a \\parallel_1 = (x_1 + x_2) = (2+3)^1=5$\n欧几里得范数\n欧几里得范数又被称作2-范数 2-norm ，是范数中最常用的范数，欧几里得范数返回的是两点之间最短的距离，因此 $\\vec{a}$ 的2-范式为 $\\parallel x \\parallel_2 = (2^2+3^2)^{\\frac{1}{2}} = (4+9)^{\\frac{1}{2}} = \\sqrt{13}$ ；用图像表示为（红线部分表示2-范数，这是 $\\vec{a}$ 表示的点到点之间的最低按距离）\n无穷范数\n无穷范数 Infinity-norm 是返回给定向量中的最大绝对值；$\\vec{a}$ 的无穷范式为 $\\parallel a \\parallel_\\infty = 3$ （公式求得是上述图中 $[2,3]$ 这个实例）。\n例如，如果我们必须找到一个向量的无穷范数，比如 $\\vec{b}$ ，$\\vec{b} = [4,3,-1]$ ；那么 $\\parallel b \\parallel_\\infty = 6$ （这里最大是4，但是返回的是一个绝对值所以是6）\nReference norm\nvector norms\n拉格朗日乘子法\n拉格朗日乘子法 Lagrange multiplier，是一种寻找受等式约束的函数的局部最大值和最小值的策略（即，必须满足一个或多个方程必须完全满足所选变量值的条件）\n设置超平面为 $wx+b=0$ ，其中 $w=[1,2,\\ ..,\\ n]$ ，w是 $n \\times 1$ 维，n特征值的个数，x 训练的示例，b是bias，一个二维的超平面的特征为：$x=[x_1,x_2]$ ，$w=[w_1,w_2]$ ，b看做 wegiht $w_0$ ，\n那么这个超平面的方程就为：\n$$ f(n) \\begin{cases} w_1x_1+w_2x_2+w0 = 0\\ \\ 超平面(决策边界)方程 \\\\ w_1x_1+w_2x_2+w0 \u003e 0\\ \\ 超平面(决策边界)上部分 \\\\ w_1x_1+w_2x_2+w0 \u003c 0\\ \\ 超平面(决策边界)下部分 \\\\ \\end{cases} $$ ，那么在对公式进行分解，增加参数 y ，代表了对向量的分类，也就是说超平面两边的向量，这样公式为：\n$$ f(n) \\begin{cases} w_1x_1+w_2x_2+w0 \\ge 1\\ \\ 当 y_i = +1 \\\\ w_1x_1+w_2x_2+w0 \\le 1\\ \\ 当 y_i = -1 \\\\ \\end{cases} $$ ","wordCount":"342","inLanguage":"zh","datePublished":"2022-06-01T00:00:00Z","dateModified":"2023-03-22T23:00:36+08:00","author":{"@type":"Person","name":"cylon"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.oomkill.com/2022/06/decision-boundary/"},"publisher":{"@type":"Organization","name":"Cylon's Collection","logo":{"@type":"ImageObject","url":"https://www.oomkill.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.oomkill.com/><img src=https://www.oomkill.com/favicon.ico alt aria-label=logo height=20>Cylon's Collection</a><div class=logo-switches><button id=theme-toggle><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.oomkill.com/archives><span>归档</span></a></li><li><a href=https://www.oomkill.com/tags><span>标签</span></a></li><li><a href=https://www.oomkill.com/search><span>搜索</span></a></li><li><a href=https://www.oomkill.com/about accesskey=/><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">决策边界算法</h1><div class=post-meta><span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>2022-06-01</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg><span>342 字</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><span>2 分钟</span></span>
<span class=pe-post-meta-item>&nbsp;·&nbsp;<svg t="1714036239378" fill="currentcolor" class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6659" width="256" height="256"><path d="M690 78.2c-18.6-18.8-49-19-67.8-.4s-19 49-.4 67.8l255.4 258.6c67.8 68.6 67.8 178.8.0 247.4L653.4 878.2c-18.6 18.8-18.4 49.2.4 67.8s49.2 18.4 67.8-.4l224-226.4c104.8-106 104.8-276.4.0-382.4L690 78.2zM485.4 101.4c-24-24-56.6-37.4-90.6-37.4H96C43 64 0 107 0 160v299c0 34 13.4 66.6 37.4 90.6l336 336c50 50 131 50 181 0l267-267c50-50 50-131 0-181l-336-336zM96 160h299c8.4.0 16.6 3.4 22.6 9.4l336 336c12.4 12.4 12.4 32.8.0 45.2l-267 267c-12.4 12.4-32.8 12.4-45.2.0l-336-336c-6-6-9.4-14.2-9.4-22.6V160zm192 128a64 64 0 10-128 0 64 64 0 10128 0z" p-id="6660"/></svg></span><ul class=pe-post-meta-item><a href=https://www.oomkill.com/tags/machinelearning/>#MachineLearning</a>
<a href=https://www.oomkill.com/tags/algorithm/>#Algorithm</a>
<a href=https://www.oomkill.com/tags/cs/>#CS</a></ul></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details><summary><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e4%bb%80%e4%b9%88%e6%98%af%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f aria-label=什么是支持向量>什么是支持向量</a><li><a href=#reference aria-label=Reference>Reference</a></li></div></details></div></aside><script src=/js/pe-toc.min.445eb1bfc5e85dd13b9519fcc2a806522e9629b6224a2974052789ba00ab78af.js integrity="sha256-RF6xv8XoXdE7lRn8wqgGUi6WKbYiSil0BSeJugCreK8="></script><div class=post-content><p><strong>决策边界</strong> (<code>decision boundary</code>)</p><p>支持向量机获取这些数据点并输出最能分离标签的超平面。这条线是<strong>决策边界</strong></p><p>决策平面 （ <code>decision surface</code> ），是将空间划分为不同的区域。位于决策平面一侧的数据被定义为与位于另一侧的数据属于不同的类别。决策面可以作为学习过程的结果创建或修改，它们经常用于机器学习、模式识别和分类系统。</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/Using-Eq-18-as-decision-surface-for-classifying-with-two-overlapped-data-classes.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/Using-Eq-18-as-decision-surface-for-classifying-with-two-overlapped-data-classes.png#center alt="Using Eq. (18) as decision surface for classifying, with two overlapped data classes" onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><p>环境空间 ( <code>Ambient Space</code>)，围绕数学对象即对象本身的空间，如一维 <code>Line</code> ，可以独立研究，这种情况下L则是L；再例如将L作为二维空间 $R^2$ 的对象进行研究，这种情况下 <strong>L</strong> 的环境空间是 $R^2$。</p><p>超平面（<code>Hyperplane</code>）是一个子空间， N维空间的超平面是其具有维数的平面的子集。就其性质而言，它将空间分成两个半空间，其维度比其<strong>环境空间</strong>的维度小 <strong>1</strong>。如果空间是三维的，那么它的超平面就是二维维平面，而如果空间是 2 维的，那么它的超平面就是一维线。支持向量机 (SVM) 通过找到使两个类之间的边距最大化的超平面来执行分类。</p><p>法向量 （<code>Normal</code>） 是垂直于该平面、另一个向量的 90° 角倾斜</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/220px-Normal_vectors2.svg.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/220px-Normal_vectors2.svg.png#center alt=img onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><h2 id=什么是支持向量>什么是支持向量<a hidden class=anchor aria-hidden=true href=#什么是支持向量>#</a></h2><p>支持向量 （<code>Support vectors</code>），靠近决策平面（超平面）的数据点。</p><p>如图所示，从一维平面来看，哪个是分离的超平面？</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-20220530222414946.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-20220530222414946.png#center alt=image-20220530222414946 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><p>一般而言，会有很多种解决方法（超平面），支持向量机就是如何找到最佳方法的解决方案。</p><p>转置运算</p><p>矩阵的转置是原始矩阵的翻转版本，可以通过转换矩阵的行和列来转置矩阵。我们用 $A^T$ 表示矩阵 A 的转置。例如，</p>$$A=\left[
\begin{matrix}
1 & 2 & 3 \\
   4 & 5 & 6 \\
  \end{matrix}
\right]$$<p>；那么 A 的转置就为</p>$$A=\left[
\begin{matrix}
1 & 4 \\
   2 & 5 \\
   3 & 6 \\
  \end{matrix}
\right]$$<p>；</p><p>我们可以将向量的转置作为特例。由于 n 维向量 x 由 n×1 列矩阵表示：</p>$$x=\left[
\begin{matrix}
x_1 \\
x_2 \\
x_3 \\
.... \\
x_n \\
\end{matrix}
\right]$$<p>；那么 <strong>x</strong> 的转置（$x^T$）是一个 $1\times n$ 行矩阵</p>$$x^T=\left[
\begin{matrix}
x_1 & x_2 & x_3 & ... & x_n \\
  \end{matrix}
\right]$$<p>。</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/200px-Matrix_transpose-16539874514953.gif><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/200px-Matrix_transpose-16539874514953.gif#center alt=img onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><p>权重向量</p><p>$wx+b=0$ w：权重向量 x n维向量 $x_i=[1,2,3&mldr;n]$ $w_i=[1,2,3&mldr;n]$ 每个输入的值都乘以一个“权重” $w_i$。权重是表示计算输出时每个输入的重要性的值</p><p>权重决定了输入对输出的影响程度。 $Y=\sum(Weight \times input)+bias$ ；如果输入为 $[x_1,x_2\ &mldr; ,x_n]$ 权重是：$[w_1,w_2\ \ ,w_n]$</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/assets%252F-LvBP1svpACTB1R1x_U4%252F-LvI8vNq_N7u3RWVAPLk%252F-LvJSdcFXzoI-WW0L3w5%252Fimage.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/assets%252F-LvBP1svpACTB1R1x_U4%252F-LvI8vNq_N7u3RWVAPLk%252F-LvJSdcFXzoI-WW0L3w5%252Fimage.png#center alt=img onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><p>通过场景来理解</p><p>假设预估汽车的价格，汽车的价格取决于制造年份和行驶里程数。让我们假设汽车的年份越高，汽车价格越高。随后，汽车开得越多，汽车就越便宜。</p><p>这个例子应该可以帮助您了解汽车价格与制造年份之间存在正相关关系，而汽车价格与其行驶里程之间存在负关关系。因此，我们希望看到代表年份的特征的权重为正，代表里程的特征的权重为负。公式为：$car = (w_1x\ ear+w_2x\ miles)$</p><p>偏差 <strong>bais</strong> 是一个常数 <code>const</code> ，偏差用于将影响函数的结果向正或负方向移动。<code>bias</code> 会被被添加到 <strong>input</strong> 和 <strong>weight</strong> 的乘积中。偏差用于抵消结果。$x_1w_1+x_2w_2&mldr;x_nw_n+bias$</p><p>通过场景来理解</p><p>假设希望在输入为 0 时返回 2。由于权重和输入的乘积之和为 0，您将如何确保返回 2？<strong>此时可以添加2的bias</strong>。如果不包含偏差，只是对 <strong>input</strong> 和 <strong>weight</strong> 执行矩阵乘法。这将很容易导致过度拟合数据集。</p><blockquote><p><strong>过度拟合</strong>（overfitting）是指机器学习算模型在训练集上的误差和测试集上的误差之间差异过大。造成过度拟合的原因可能有多种．最常见的就是模型容量过高，模型过于复杂，换句话说是模型假设所包含的参数数量过多．如此一来，算法会将训练集中所包含的没有普遍性的一些特征也学习进来，结果降低了模型的泛化能力．</p></blockquote><p><a href=https://machine-learning.paperspace.com/wiki/weights-and-biases target=_blank rel="noopener nofollow noreferrer">https://machine-learning.paperspace.com/wiki/weights-and-biases</a></p><p>范数</p><p>向量的范数（<strong>norm</strong>）是它的长度 ，x的范数表示为 $\parallel x \parallel$；常用的范数为 P 范数，其中 P 是大于等于1的任何数，向量 <strong>x</strong>的 p 范数表示为 $\parallel x \parallel_p$ ；通常情况下向量 x 的 p 范数的计算公式为： $\parallel x \parallel_p = (x_1^p+x_2^p+x_3^p+ \ &mldr;\ x_n^p)^{1/p}$ ；公式可以简写为：$\parallel x \parallel_p = (\sum_{i=1}^n\ x_i^p)^{1/p}$</p><p>曼哈顿距离</p><p>曼哈顿距离也被称为1-范数 <code>1-norm</code>，因为它测量的中两点之间的距离。假设：向量a，我们必须计算 1-范式 $\vec{a} = [2,3]$ ，在图像中表示（红色线部分表示向量a的1-范式）</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/1PU0J-FJWvTj37huxQjWy8g.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/1PU0J-FJWvTj37huxQjWy8g.png#center alt=1-norm onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><p>通过公式来计算1-范式，可以将p替换为1，$\parallel a \parallel_1 = (x_1 + x_2) = (2+3)^1=5$</p><p>欧几里得范数</p><p>欧几里得范数又被称作2-范数 <code>2-norm</code> ，是范数中最常用的范数，欧几里得范数返回的是两点之间最短的距离，因此 $\vec{a}$ 的2-范式为 $\parallel x \parallel_2 = (2^2+3^2)^{\frac{1}{2}} = (4+9)^{\frac{1}{2}} = \sqrt{13}$ ；用图像表示为（红线部分表示2-范数，这是 $\vec{a}$ 表示的点到点之间的最低按距离）</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/1tFNCvthlEe_ajinO4ngtEg.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/1tFNCvthlEe_ajinO4ngtEg.png#center alt=2-norm onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><p>无穷范数</p><p>无穷范数 <code>Infinity-norm</code> 是返回给定向量中的最大绝对值；$\vec{a}$ 的无穷范式为 $\parallel a \parallel_\infty = 3$ （公式求得是上述图中 $[2,3]$ 这个实例）。</p><p>例如，如果我们必须找到一个向量的无穷范数，比如 $\vec{b}$ ，$\vec{b} = [4,3,-1]$ ；那么 $\parallel b \parallel_\infty = 6$ （这里最大是4，但是返回的是一个绝对值所以是<strong>6</strong>）</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><blockquote><p><a href=https://medium.com/linear-algebra/part-18-norms-30a8b3739bb target=_blank rel="noopener nofollow noreferrer">norm</a></p><p><a href=https://machinelearningmastery.com/vector-norms-machine-learning/ target=_blank rel="noopener nofollow noreferrer">vector norms</a></p></blockquote><p><strong>拉格朗日乘子法</strong></p><p><strong>拉格朗日乘子法</strong> <code>Lagrange multiplier</code>，是一种寻找受<a href=https://en.wikipedia.org/wiki/Constraint_%28mathematics%29 target=_blank rel="noopener nofollow noreferrer">等式约束</a>的<a href=https://en.wikipedia.org/wiki/Function_%28mathematics%29 target=_blank rel="noopener nofollow noreferrer">函数的局部</a><a href=https://en.wikipedia.org/wiki/Maxima_and_minima target=_blank rel="noopener nofollow noreferrer">最大值和最小值</a>的策略（即，必须满足一个或多个<a href=https://en.wikipedia.org/wiki/Equation target=_blank rel="noopener nofollow noreferrer">方程</a>必须完全满足所选<a href=https://en.wikipedia.org/wiki/Variable_%28mathematics%29 target=_blank rel="noopener nofollow noreferrer">变量</a>值的条件）</p><p>设置超平面为 $wx+b=0$ ，其中 $w=[1,2,\ ..,\ n]$ ，w是 $n \times 1$ 维，n特征值的个数，x 训练的示例，b是bias，一个二维的超平面的特征为：$x=[x_1,x_2]$ ，$w=[w_1,w_2]$ ，b看做 wegiht $w_0$ ，</p><p>那么这个超平面的方程就为：</p>$$
f(n)
\begin{cases}
w_1x_1+w_2x_2+w0 = 0\ \ 超平面(决策边界)方程 \\
w_1x_1+w_2x_2+w0 > 0\ \ 超平面(决策边界)上部分 \\
w_1x_1+w_2x_2+w0 < 0\ \ 超平面(决策边界)下部分 \\
\end{cases}
$$<p>，那么在对公式进行分解，增加参数 <code>y</code> ，代表了对向量的分类，也就是说超平面两边的向量，这样公式为：</p>$$
f(n)
\begin{cases}
w_1x_1+w_2x_2+w0 \ge 1\ \ 当 y_i = +1 \\
w_1x_1+w_2x_2+w0 \le 1\ \ 当 y_i = -1 \\
\end{cases}
$$</div><div class=pe-copyright><hr><blockquote><p>本文为原创内容，版权归作者所有。如需转载，请在文章中声明本文标题及链接。</p><p>文章标题：决策边界算法</p><p>文章链接：<a href=https://www.oomkill.com/2022/06/decision-boundary/ target=_blank>https://www.oomkill.com/2022/06/decision-boundary/</a></p><p>许可协议：<a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></p></blockquote></div><div class=comments-separator></div><h3 class=relatedContentTitle>相关阅读</h3><ul class=relatedContent><li><a href=/2022/06/knn/><span>KNN算法</span></a></li><li><a href=/2022/06/decision-tree/><span>决策树</span></a></li><li><a href=/2022/06/naive-bayes/><span>朴素贝叶斯算法</span></a></li><li><a href=/2022/06/logistic-regression/><span>逻辑回归</span></a></li><li><a href=/2016/09/consistent-hash/><span>一致性hash在memcache中的应用</span></a></li></ul><div class=comments-separator></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.oomkill.com/tags/machinelearning/>MachineLearning</a></li><li><a href=https://www.oomkill.com/tags/algorithm/>Algorithm</a></li><li><a href=https://www.oomkill.com/tags/cs/>CS</a></li></ul><nav class=paginav><a class=prev href=https://www.oomkill.com/2022/06/knn/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></polyline></svg>&nbsp;</span>
<span>KNN算法</span>
</a><a class=next href=https://www.oomkill.com/2022/06/decision-tree/><span class=title></span>
<span>决策树&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span></a></nav></footer><div class=pe-comments-decoration><p class=pe-comments-title></p><p class=pe-comments-subtitle></p></div><div id=pe-comments></div><script src=/js/pe-go-comment.min.86a214102576ba5f9b7bdc29eed8d58dd56e34aef80b3c65c73ea9cc88443696.js integrity="sha256-hqIUECV2ul+be9wp7tjVjdVuNK74Czxlxz6pzIhENpY="></script><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="dark"?"dark":"light",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"cylonchau/cylonchau.github.io","data-repo-id":"R_kgDOIRlNSQ","data-category":"Announcements","data-category-id":"DIC_kwDOIRlNSc4CXy1U","data-mapping":"pathname","data-term":"posts/decision boundary","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":getStoredTheme(),"data-lang":"zh-TW","data-loading":"lazy",crossorigin:"anonymous",async:""},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#pe-comments").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2024 <a href=https://www.oomkill.com/>Cylon's Collection</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> on
<a href=https://pages.github.com/ rel=noopener target=_blank>GitHub Pages</a> & Theme
        <a href=https://github.com/tofuwine/PaperMod-PE rel=noopener target=_blank>PaperMod-PE</a></span></footer><div class=pe-right-sidebar><a href=javascript:void(0); id=theme-toggle-float class=pe-float-btn><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</a><a href=#top class=pe-float-btn id=top-link><span id=pe-read-progress></span></a></div><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>